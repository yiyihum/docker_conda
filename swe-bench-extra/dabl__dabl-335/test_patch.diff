diff --git a/dabl/plot/tests/test_supervised.py b/dabl/plot/tests/test_supervised.py
index 57b4fe3..4718687 100644
--- a/dabl/plot/tests/test_supervised.py
+++ b/dabl/plot/tests/test_supervised.py
@@ -27,27 +27,38 @@ from dabl import set_config
 def test_plots_smoke(continuous_features, categorical_features, task):
     # simple smoke test
     # should be parametrized
+    if continuous_features == 0 and categorical_features == 0:
+        pytest.skip("Need at least one feature")
     n_samples = 100
-    X_cont, y_cont = make_regression(
-        n_samples=n_samples, n_features=continuous_features,
-        n_informative=min(continuous_features, 2))
-    X_cat, y_cat = make_regression(
-        n_samples=n_samples, n_features=categorical_features,
-        n_informative=min(categorical_features, 2))
-    if X_cat.shape[1] > 0:
-        X_cat = KBinsDiscretizer(encode='ordinal').fit_transform(X_cat)
-    cont_columns = ["asdf_%d_cont" % i for i in range(continuous_features)]
-    df_cont = pd.DataFrame(X_cont, columns=cont_columns)
+    if continuous_features > 0:
+        X_cont, y_cont = make_regression(
+            n_samples=n_samples, n_features=continuous_features,
+            n_informative=min(continuous_features, 2))
+    if categorical_features > 0:
+        X_cat, y_cat = make_regression(
+            n_samples=n_samples, n_features=categorical_features,
+            n_informative=min(categorical_features, 2))
+    if continuous_features > 0:
+        cont_columns = ["asdf_%d_cont" % i for i in range(continuous_features)]
+        df_cont = pd.DataFrame(X_cont, columns=cont_columns)
     if categorical_features > 0:
+        X_cat = KBinsDiscretizer(encode='ordinal').fit_transform(X_cat)
         cat_columns = ["asdf_%d_cat" % i for i in range(categorical_features)]
         df_cat = pd.DataFrame(X_cat, columns=cat_columns).astype('int')
         df_cat = df_cat.astype("category")
+    if categorical_features > 0 and continuous_features > 0:
         X_df = pd.concat([df_cont, df_cat], axis=1)
-    else:
+        y = y_cont + y_cat
+    elif categorical_features > 0:
+        X_df = df_cat
+        y = y_cat
+    elif continuous_features > 0:
         X_df = df_cont
+        y = y_cont
+    else:
+        raise ValueError("invalid")
     assert X_df.shape[1] == continuous_features + categorical_features
     X_clean = clean(X_df.copy())
-    y = y_cont + y_cat
     if X_df.shape[1] == 0:
         y = np.random.uniform(size=n_samples)
     if task == "classification":
@@ -73,9 +84,9 @@ def test_plots_smoke(continuous_features, categorical_features, task):
                                            ['continuous', 'categorical'],
                                            ['continuous', 'categorical']))
 def test_type_hints(add, feature_type, target_type):
-    X = pd.DataFrame(np.random.randint(4, size=100)) + add
+    X = pd.DataFrame(np.random.randint(4, size=100), columns=['feat']) + add
     X['target'] = np.random.uniform(size=100)
-    plot(X, type_hints={0: feature_type,
+    plot(X, type_hints={'feat': feature_type,
                         'target': target_type},
          target_col='target')
     # get title of figure
diff --git a/dabl/tests/test_available_if.py b/dabl/tests/test_available_if.py
new file mode 100644
index 0000000..66fc221
--- /dev/null
+++ b/dabl/tests/test_available_if.py
@@ -0,0 +1,67 @@
+# Note: As a part of dabl PR #335, this file was copied from
+# sklearn/utils/tests/test_metaestimators.py from the scikit-learn repo at commit
+# e2b3785b6a1f96989c3992bffa2a05ef5c048a7e
+
+import pickle
+
+import pytest
+
+from dabl._available_if import available_if
+
+
+class AvailableParameterEstimator:
+    """This estimator's `available` parameter toggles the presence of a method"""
+
+    def __init__(self, available=True, return_value=1):
+        self.available = available
+        self.return_value = return_value
+
+    @available_if(lambda est: est.available)
+    def available_func(self):
+        """This is a mock available_if function"""
+        return self.return_value
+
+
+def test_available_if_docstring():
+    assert "This is a mock available_if function" in str(
+        AvailableParameterEstimator.__dict__["available_func"].__doc__
+    )
+    assert "This is a mock available_if function" in str(
+        AvailableParameterEstimator.available_func.__doc__
+    )
+    assert "This is a mock available_if function" in str(
+        AvailableParameterEstimator().available_func.__doc__
+    )
+
+
+def test_available_if():
+    assert hasattr(AvailableParameterEstimator(), "available_func")
+    assert not hasattr(AvailableParameterEstimator(available=False), "available_func")
+
+
+def test_available_if_unbound_method():
+    # This is a non regression test for:
+    # https://github.com/scikit-learn/scikit-learn/issues/20614
+    # to make sure that decorated functions can be used as an unbound method,
+    # for instance when monkeypatching.
+    est = AvailableParameterEstimator()
+    AvailableParameterEstimator.available_func(est)
+
+    est = AvailableParameterEstimator(available=False)
+    with pytest.raises(
+        AttributeError,
+        match="This 'AvailableParameterEstimator' has no attribute 'available_func'",
+    ):
+        AvailableParameterEstimator.available_func(est)
+
+
+def test_available_if_methods_can_be_pickled():
+    """Check that available_if methods can be pickled.
+
+    Non-regression test for #21344.
+    """
+    return_value = 10
+    est = AvailableParameterEstimator(available=True, return_value=return_value)
+    pickled_bytes = pickle.dumps(est.available_func)
+    unpickled_func = pickle.loads(pickled_bytes)
+    assert unpickled_func() == return_value
