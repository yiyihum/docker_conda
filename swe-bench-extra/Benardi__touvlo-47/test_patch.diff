diff --git a/tests/supv/test_lin_rg.py b/tests/supv/test_lin_rg.py
index 001047e..4f452b3 100644
--- a/tests/supv/test_lin_rg.py
+++ b/tests/supv/test_lin_rg.py
@@ -3,9 +3,11 @@ import os
 import pytest
 from numpy.testing import assert_allclose
 from numpy import ones, zeros, float64, array, append, genfromtxt
+from numpy.linalg import LinAlgError
 
 from touvlo.supv.lin_rg import (normal_eqn, cost_func, reg_cost_func, grad,
-                                reg_grad, predict, h)
+                                reg_grad, predict, h, LinearRegression,
+                                RidgeLinearRegression, reg_normal_eqn)
 from touvlo.utils import numerical_grad
 
 TESTDATA1 = os.path.join(os.path.dirname(__file__), 'data1.csv')
@@ -33,7 +35,7 @@ class TestLinearRegression:
 
         y = data1[:, -1:]
         X = data1[:, :-1]
-        m, n = X.shape
+        m, _ = X.shape
         intercept = ones((m, 1), dtype=int)
         X = append(intercept, X, axis=1)
 
@@ -44,7 +46,7 @@ class TestLinearRegression:
     def test_normal_eqn_data2(self, data2):
         y = data2[:, -1:]
         X = data2[:, :-1]
-        m, n = X.shape
+        m, _ = X.shape
         intercept = ones((m, 1), dtype=int)
         X = append(intercept, X, axis=1)
 
@@ -52,6 +54,81 @@ class TestLinearRegression:
                         normal_eqn(X, y),
                         rtol=0, atol=0.001)
 
+    def test_reg_normal_eqn_data1_1(self, data1):
+
+        y = data1[:, -1:]
+        X = data1[:, :-1]
+        m, _ = X.shape
+        intercept = ones((m, 1), dtype=int)
+        X = append(intercept, X, axis=1)
+        _lambda = 0
+
+        assert_allclose([[-3.896], [1.193]],
+                        reg_normal_eqn(X, y, _lambda),
+                        rtol=0, atol=0.001)
+
+    def test_reg_normal_eqn_data1_2(self, data1):
+
+        y = data1[:, -1:]
+        X = data1[:, :-1]
+        m, _ = X.shape
+        intercept = ones((m, 1), dtype=int)
+        X = append(intercept, X, axis=1)
+        _lambda = 1
+
+        assert_allclose([[-3.889], [1.192]],
+                        reg_normal_eqn(X, y, _lambda),
+                        rtol=0, atol=0.001)
+
+    def test_reg_normal_eqn_data2(self, data2):
+        y = data2[:, -1:]
+        X = data2[:, :-1]
+        m, _ = X.shape
+        intercept = ones((m, 1), dtype=int)
+        X = append(intercept, X, axis=1)
+        _lambda = 100
+
+        assert_allclose([[74104.492], [135.249], [-1350.731]],
+                        reg_normal_eqn(X, y, _lambda),
+                        rtol=0, atol=0.001)
+
+    def test_normal_eqn_singular(self, data2):
+        y = array([[0], [0], [0]])
+        X = array([[0, 0, 0], [0, 0, 0], [0, 0, 0]])
+        m, _ = X.shape
+        intercept = ones((m, 1), dtype=int)
+        X = append(intercept, X, axis=1)
+
+        with pytest.raises(LinAlgError) as excinfo:
+            normal_eqn(X, y)
+        msg = excinfo.value.args[0]
+        assert msg == ("Singular matrix")
+
+    def test_reg_normal_eqn_singular1(self, data2):
+        y = array([[0], [0], [0]])
+        X = array([[0, 0, 0], [0, 0, 0], [0, 0, 0]])
+        m, _ = X.shape
+        intercept = ones((m, 1), dtype=int)
+        X = append(intercept, X, axis=1)
+        _lambda = 0
+
+        with pytest.raises(LinAlgError) as excinfo:
+            reg_normal_eqn(X, y, _lambda),
+        msg = excinfo.value.args[0]
+        assert msg == ("Singular matrix")
+
+    def test_reg_normal_eqn_singular2(self, data2):
+        y = array([[0], [0], [0]])
+        X = array([[0, 0, 0], [0, 0, 0], [0, 0, 0]])
+        m, _ = X.shape
+        intercept = ones((m, 1), dtype=int)
+        X = append(intercept, X, axis=1)
+        _lambda = 0.1
+
+        assert_allclose([[0], [0], [0], [0]],
+                        reg_normal_eqn(X, y, _lambda),
+                        rtol=0, atol=0.001)
+
 # COST FUNCTION
 
     def test_cost_func_data1_1(self, data1):
@@ -505,7 +582,7 @@ class TestLinearRegression:
 
     def test_h_1(self):
         X = array([[3.5]])
-        m, n = X.shape
+        m, _ = X.shape
         intercept = ones((m, 1), dtype=float64)
         X = append(intercept, X, axis=1)
         theta = array([[-3.6303], [1.1664]])
@@ -546,3 +623,288 @@ class TestLinearRegression:
         assert_allclose([[-0.2]],
                         h(X, theta),
                         rtol=0, atol=0.001)
+
+# LINEAR REGRESSION CLASS
+
+    def test_LinearRegression_constructor1(self, data1):
+        theta = array([[1.], [0.6], [1.]])
+        lr = LinearRegression(theta)
+
+        assert_allclose(array([[1.], [0.6], [1.]]),
+                        lr.theta,
+                        rtol=0, atol=0.001)
+
+    def test_LinearRegression_constructor2(self):
+        lr = LinearRegression()
+
+        assert lr.theta is None
+
+    def test_LinearRegression_cost_data1_1(self, data1):
+        y = data1[:, -1:]
+        X = data1[:, :-1]
+        _, n = X.shape
+        theta = ones((n + 1, 1), dtype=float64)
+        lr = LinearRegression(theta)
+
+        assert_allclose([[10.266]],
+                        lr.cost(X, y),
+                        rtol=0, atol=0.001)
+
+    def test_LinearRegression_normal_fit(self, data1):
+        y = data1[:, -1:]
+        X = data1[:, :-1]
+        lr = LinearRegression()
+        lr.fit(X, y, strategy="normal_equation")
+
+        assert_allclose([[-3.896], [1.193]],
+                        lr.theta,
+                        rtol=0, atol=0.001)
+
+    def test_LinearRegression_fit_BGD(self, data2):
+        y = data2[:, -1:]
+        X = data2[:, :-1]
+        lr = LinearRegression()
+        lr.fit(X, y, strategy="BGD", alpha=1, num_iters=1)
+
+        assert_allclose([[340412.659], [764209128.191], [1120367.702]],
+                        lr.theta,
+                        rtol=0, atol=0.001)
+
+    def test_LinearRegression_fit_SGD(self, err):
+        X = array([[0, 1, 2], [-1, 5, 3], [2, 0, 1]])
+        y = array([[0.3], [1.2], [0.5]])
+        lr = LinearRegression()
+        lr.fit(X, y, strategy="SGD", alpha=1, num_iters=1)
+
+        assert_allclose(array([[2.3], [11.2], [-11.7], [-2.2]]),
+                        lr.theta,
+                        rtol=0, atol=0.001, equal_nan=False)
+
+    def test_LinearRegression_fit_MBGD1(self, data2):
+        y = data2[:, -1:]
+        X = data2[:, :-1]
+        m = len(X)
+        lr = LinearRegression()
+        lr.fit(X, y, strategy="MBGD", alpha=1, num_iters=1, b=m)
+
+        assert_allclose([[340412.659], [764209128.191], [1120367.702]],
+                        lr.theta,
+                        rtol=0, atol=0.001)
+
+    def test_LinearRegression_fit_MBGD2(self, err):
+        X = array([[0, 1, 2], [-1, 5, 3], [2, 0, 1]])
+        y = array([[0.3], [1.2], [0.5]])
+        lr = LinearRegression()
+        lr.fit(X, y, strategy="MBGD", alpha=1, num_iters=1, b=1)
+
+        assert_allclose(array([[2.3], [11.2], [-11.7], [-2.2]]),
+                        lr.theta,
+                        rtol=0, atol=0.001, equal_nan=False)
+
+    def test_LinearRegression_predict(self):
+        X = array([[3.5]])
+        theta = array([[-3.6303], [1.1664]])
+        lr = LinearRegression(theta)
+
+        assert_allclose([[0.4521]],
+                        lr.predict(X),
+                        rtol=0, atol=0.001)
+
+# RIDGE LINEAR REGRESSION CLASS
+
+    def test_RidgeLinearRegression_constructor1(self, data1):
+        theta = ones((3, 1), dtype=float64)
+        lr = RidgeLinearRegression(theta, _lambda=13.50)
+
+        assert lr._lambda == 13.50
+        assert_allclose(array([[1.], [1.], [1.]]),
+                        lr.theta,
+                        rtol=0, atol=0.001)
+
+    def test_RidgeLinearRegression_constructor2(self):
+        lr = RidgeLinearRegression()
+
+        assert lr.theta is None
+        assert lr._lambda == 0
+
+    def test_RidgeLinearRegression_cost_data1_1(self, data1):
+        y = data1[:, -1:]
+        X = data1[:, :-1]
+        _, n = X.shape
+        theta = ones((n + 1, 1), dtype=float64)
+        lr = RidgeLinearRegression(theta, _lambda=0)
+
+        assert_allclose([[10.266]],
+                        lr.cost(X, y),
+                        rtol=0, atol=0.001)
+
+    def test_RidgeLinearRegression_cost_data1_2(self, data1):
+
+        y = data1[:, -1:]
+        X = data1[:, :-1]
+        _, n = X.shape
+        theta = ones((n + 1, 1), dtype=float64)
+        lr = RidgeLinearRegression(theta, _lambda=100)
+
+        assert_allclose([[10.781984]],
+                        lr.cost(X, y),
+                        rtol=0, atol=0.001)
+
+    def test_RidgeLinearRegression_cost_data2_1(self, data2):
+        y = data2[:, -1:]
+        X = data2[:, :-1]
+        _, n = X.shape
+        theta = ones((n + 1, 1), dtype=float64)
+        lr = RidgeLinearRegression(theta, _lambda=0)
+
+        assert_allclose([[64828197300.798]],
+                        lr.cost(X, y),
+                        rtol=0, atol=0.001)
+
+    def test_RidgeLinearRegression_cost_data2_2(self, data2):
+        y = data2[:, -1:]
+        X = data2[:, :-1]
+        _, n = X.shape
+        theta = ones((n + 1, 1), dtype=float64)
+        lr = RidgeLinearRegression(theta, _lambda=1000000)
+
+        assert_allclose([[64828218577.393623]],
+                        lr.cost(X, y),
+                        rtol=0, atol=0.001)
+
+    def test_RidgeLinearRegression_normal_fit_data1_1(self, data1):
+        y = data1[:, -1:]
+        X = data1[:, :-1]
+        lr = RidgeLinearRegression(_lambda=0)
+        lr.fit(X, y, strategy="normal_equation")
+
+        assert_allclose([[-3.896], [1.193]],
+                        lr.theta,
+                        rtol=0, atol=0.001)
+
+    def test_RidgeLinearRegression_normal_fit_data1_2(self, data1):
+        y = data1[:, -1:]
+        X = data1[:, :-1]
+        lr = RidgeLinearRegression(_lambda=1)
+        lr.fit(X, y, strategy="normal_equation")
+
+        assert_allclose([[-3.889], [1.192]],
+                        lr.theta,
+                        rtol=0, atol=0.001)
+
+    def test_RidgeLinearRegression_fit_BGD1(self, data2):
+        y = data2[:, -1:]
+        X = data2[:, :-1]
+        lr = RidgeLinearRegression(_lambda=0)
+        lr.fit(X, y, strategy="BGD", alpha=1, num_iters=1)
+
+        assert_allclose([[340412.659], [764209128.191], [1120367.702]],
+                        lr.theta,
+                        rtol=0, atol=0.001)
+
+    def test_RidgeLinearRegression_fit_BGD2(self, data1):
+        y = data1[:, -1:]
+        X = data1[:, :-1]
+        _, n = X.shape
+        theta = ones((n + 1, 1), dtype=float64)
+        lr = RidgeLinearRegression(theta, _lambda=100)
+
+        lr.fit(X, y, strategy="BGD", alpha=1, num_iters=1)
+
+        assert_allclose([[-2.321], [-24.266]],
+                        lr.theta,
+                        rtol=0, atol=0.001)
+
+    def test_RidgeLinearRegression_fit_SGD1(self, err):
+        X = array([[0, 1, 2], [-1, 5, 3], [2, 0, 1]])
+        y = array([[0.3], [1.2], [0.5]])
+        lr = RidgeLinearRegression(_lambda=0)
+        lr.fit(X, y, strategy="SGD", alpha=1, num_iters=1)
+
+        assert_allclose(array([[2.3], [11.2], [-11.7], [-2.2]]),
+                        lr.theta,
+                        rtol=0, atol=0.001, equal_nan=False)
+
+    def test_RidgeLinearRegression_fit_SGD2(self, err):
+        X = array([[0, 1, 2], [-1, 5, 3], [2, 0, 1]])
+        y = array([[0.3], [1.2], [0.5]])
+        lr = RidgeLinearRegression(_lambda=10)
+        lr.fit(X, y, strategy="SGD", alpha=1, num_iters=1)
+
+        assert_allclose(array([[8.3], [-0.8], [132.3], [123.8]]),
+                        lr.theta,
+                        rtol=0, atol=0.001, equal_nan=False)
+
+    def test_RidgeLinearRegression_fit_MBGD1(self, data2):
+        y = data2[:, -1:]
+        X = data2[:, :-1]
+        m = len(X)
+        lr = RidgeLinearRegression(_lambda=0)
+        lr.fit(X, y, strategy="MBGD", alpha=1, num_iters=1, b=m)
+
+        assert_allclose([[340412.659], [764209128.191], [1120367.702]],
+                        lr.theta,
+                        rtol=0, atol=0.001)
+
+    def test_RidgeLinearRegression_fit_MBGD2(self, err):
+        X = array([[0, 1, 2], [-1, 5, 3], [2, 0, 1]])
+        y = array([[0.3], [1.2], [0.5]])
+        lr = RidgeLinearRegression(_lambda=0)
+        lr.fit(X, y, strategy="MBGD", alpha=1, num_iters=1, b=1)
+
+        assert_allclose(array([[2.3], [11.2], [-11.7], [-2.2]]),
+                        lr.theta,
+                        rtol=0, atol=0.001, equal_nan=False)
+
+    def test_RidgeLinearRegression_fit_MBGD3(self, data1):
+        y = data1[:, -1:]
+        X = data1[:, :-1]
+        m, n = X.shape
+        theta = ones((n + 1, 1), dtype=float64)
+        lr = RidgeLinearRegression(theta, _lambda=100)
+        lr.fit(X, y, strategy="MBGD", alpha=1, num_iters=1, b=m)
+
+        assert_allclose([[-2.321], [-24.266]],
+                        lr.theta,
+                        rtol=0, atol=0.001)
+
+    def test_RidgeLinearRegression_fit_MBGD4(self, data1):
+        X = array([[0, 1, 2], [-1, 5, 3], [2, 0, 1]])
+        y = array([[0.3], [1.2], [0.5]])
+        lr = RidgeLinearRegression(_lambda=10)
+        lr.fit(X, y, strategy="MBGD", alpha=1, num_iters=1, b=1)
+
+        assert_allclose(array([[8.3], [-0.8], [132.3], [123.8]]),
+                        lr.theta,
+                        rtol=0, atol=0.001)
+
+    def test_RidgeLinearRegression_fit_unknown(self, data1):
+        X = array([[0, 1, 2], [-1, 5, 3], [2, 0, 1]])
+        y = array([[0.3], [1.2], [0.5]])
+        lr = RidgeLinearRegression(_lambda=10)
+
+        with pytest.raises(ValueError) as excinfo:
+            lr.fit(X, y, strategy="oompa_loompa")
+
+        msg = excinfo.value.args[0]
+        assert msg == ("'oompa_loompa' (type '<class 'str'>') was passed. ",
+                       'The strategy parameter for the fit function should ',
+                       "be 'BGD' or 'SGD' or 'MBGD' or 'normal_equation'.")
+
+    def test_RidgeLinearRegression_predict1(self):
+        X = array([[3.5]])
+        theta = array([[-3.6303], [1.1664]])
+        lr = RidgeLinearRegression(theta)
+
+        assert_allclose([[0.4521]],
+                        lr.predict(X),
+                        rtol=0, atol=0.001)
+
+    def test_RidgeLinearRegression_predict2(self):
+        X = array([[3.5]])
+        theta = array([[-3.6303], [1.1664]])
+        lr = RidgeLinearRegression(theta, _lambda=10)
+
+        assert_allclose([[0.4521]],
+                        lr.predict(X),
+                        rtol=0, atol=0.001)
diff --git a/tests/test_utils.py b/tests/test_utils.py
index f59f102..2671f02 100644
--- a/tests/test_utils.py
+++ b/tests/test_utils.py
@@ -5,7 +5,7 @@ from numpy import array, cos, sin, exp
 from numpy.testing import assert_allclose
 
 from touvlo.utils import (numerical_grad, g_grad, BGD, SGD,
-                          MBGD, mean_normlztn)
+                          MBGD, mean_normlztn, feature_normalize)
 
 
 class TestLogisticRegression:
@@ -386,3 +386,20 @@ class TestLogisticRegression:
         assert_allclose(Y_mean,
                         array([[4.5], [3], [4], [3], [3]]),
                         rtol=0, atol=0.001, equal_nan=False)
+
+    def test_feature_normalize(self, err):
+        X = array([[1.5, 8., 7.5], [1., 6., 9.]])
+        X_norm, mu, sigma = feature_normalize(X)
+
+        assert_allclose(array([1.25, 7, 8.25]),
+                        mu,
+                        rtol=0, atol=0.001, equal_nan=False)
+
+        assert_allclose(array([0.35355339, 1.41421356, 1.06066017]),
+                        sigma,
+                        rtol=0, atol=0.001, equal_nan=False)
+
+        assert_allclose(array([[0.707, 0.707, -0.707],
+                               [-0.707, -0.707, 0.707]]),
+                        X_norm,
+                        rtol=0, atol=0.001, equal_nan=False)
diff --git a/tests/unsupv/test_kmeans.py b/tests/unsupv/test_kmeans.py
index eec059d..189d62a 100644
--- a/tests/unsupv/test_kmeans.py
+++ b/tests/unsupv/test_kmeans.py
@@ -5,7 +5,7 @@ from numpy.testing import assert_allclose, assert_almost_equal
 from touvlo.unsupv.kmeans import (find_closest_centroids, euclidean_dist,
                                   compute_centroids, init_centroids,
                                   cost_function, run_kmeans,
-                                  run_intensive_kmeans)
+                                  run_intensive_kmeans, elbow_method)
 
 
 class TestKmeans:
@@ -180,3 +180,16 @@ class TestKmeans:
         assert all(idx < K)
         assert all(idx > -1)
         assert len(centroids) == K
+
+    def test_elbow_method(self):
+        X = array([[5.89562, 2.89844], [5.61754, 2.59751], [5.63176, 3.04759],
+                   [5.50259, 3.11869], [6.48213, 2.55085], [7.30279, 3.38016],
+                   [6.99198, 2.98707], [4.82553, 2.77962], [6.11768, 2.85476],
+                   [0.94049, 5.71557]])
+        max_iters = 6
+        n_inits = 1
+        K_values = [1, 2, 3, 4]
+
+        cost_values = elbow_method(X, K_values, max_iters, n_inits)
+
+        assert all(cost >= 0 for cost in cost_values)
