diff --git a/tests/test_cli.py b/tests/test_cli.py
index f6d20ae..ad10b07 100644
--- a/tests/test_cli.py
+++ b/tests/test_cli.py
@@ -1,5 +1,5 @@
 import subprocess
 
 def test_init_remote():
-    subprocess.check_output('dataflows init https://raw.githubusercontent.com/datahq/dataflows/master/data/academy.csv',
-                            shell=True)
\ No newline at end of file
+    subprocess.check_output('cd ./out && dataflows init https://raw.githubusercontent.com/datahq/dataflows/master/data/academy.csv',
+                            shell=True)
diff --git a/tests/test_lib.py b/tests/test_lib.py
index 036fa6d..d082723 100644
--- a/tests/test_lib.py
+++ b/tests/test_lib.py
@@ -1,3 +1,4 @@
+import pytest
 from dataflows import Flow
 
 data = [
@@ -19,12 +20,12 @@ def test_dump_to_sql():
                     'resource-name': 'res_1'
                 }
             ),
-            engine='sqlite:///test.db')
+            engine='sqlite:///out/test.db')
     )
     f.process()
 
     # Check validity
-    engine = create_engine('sqlite:///test.db')
+    engine = create_engine('sqlite:///out/test.db')
     result = list(dict(x) for x in engine.execute('select * from output_table'))
     assert result == data
 
@@ -438,11 +439,11 @@ def test_load_from_package():
 
     Flow(
         [{'foo': 'bar', 'moo': 12}],
-        dump_to_path('data/load_from_package')
+        dump_to_path('out/load_from_package')
     ).process()
 
     ds = Flow(
-        load('data/load_from_package/datapackage.json')
+        load('out/load_from_package/datapackage.json')
     ).datastream()
 
     assert len(ds.dp.resources) == 1
@@ -455,10 +456,10 @@ def test_load_from_env_var():
 
     Flow(
         [{'foo': 'bar'}],
-        dump_to_path('data/load_from_env_var')
+        dump_to_path('out/load_from_env_var')
     ).process()
 
-    os.environ['MY_DATAPACKAGE'] = 'data/load_from_env_var/datapackage.json'
+    os.environ['MY_DATAPACKAGE'] = 'out/load_from_env_var/datapackage.json'
     results, dp, _ = Flow(
         load('env://MY_DATAPACKAGE')
     ).results()
@@ -473,11 +474,11 @@ def test_load_from_package_resource_matching():
     Flow(
         [{'foo': 'bar'}],
         [{'foo': 'baz'}],
-        dump_to_path('data/load_from_package_resource_matching(')
+        dump_to_path('out/load_from_package_resource_matching(')
     ).process()
 
     ds = Flow(
-        load('data/load_from_package_resource_matching(/datapackage.json', resources=['res_2'])
+        load('out/load_from_package_resource_matching(/datapackage.json', resources=['res_2'])
     ).datastream()
 
     assert len(ds.dp.resources) == 1
@@ -705,10 +706,10 @@ def test_dump_to_path_use_titles():
         [{'hello': 'world', 'hola': 'mundo'}, {'hello': 'עולם', 'hola': 'عالم'}],
         *(set_type(name, resources=['res_1'], title=title) for name, title
           in (('hello', 'שלום'), ('hola', 'aloha'))),
-        dump_to_path('data/dump_with_titles', use_titles=True)
+        dump_to_path('out/dump_with_titles', use_titles=True)
     ).process()
 
-    with tabulator.Stream('data/dump_with_titles/res_1.csv') as stream:
+    with tabulator.Stream('out/dump_with_titles/res_1.csv') as stream:
         assert stream.read() == [['שלום',   'aloha'],
                                  ['world',  'mundo'],
                                  ['עולם',   'عالم']]
@@ -727,7 +728,7 @@ def test_load_dates():
             [{'today': str(_today), 'now': str(_now)}],
             set_type('today', type='date'),
             set_type('now', type='datetime', format=datetime_format),
-            dump_to_path('data/dump_dates')
+            dump_to_path('out/dump_dates')
         ).process()
 
     try:
@@ -748,7 +749,7 @@ def test_load_dates():
     out_now = datetime.datetime(_now.year, _now.month, _now.day, _now.hour, _now.minute, _now.second)
 
     assert Flow(
-        load('data/dump_dates/datapackage.json'),
+        load('out/dump_dates/datapackage.json'),
     ).results()[0] == [[{'today': _today, 'now': out_now}]]
 
 
@@ -900,11 +901,11 @@ def test_save_load_dates():
         [{'id': 1, 'ts': datetime.datetime.now()},
          {'id': 2, 'ts': datetime.datetime.now()}],
         set_type('ts', type='datetime', format='%Y-%m-%d/%H:%M:%S'),
-        dump_to_path('data/test_save_load_dates')
+        dump_to_path('out/test_save_load_dates')
     ).process()
 
     res, _, _ = Flow(
-        load('data/test_save_load_dates/datapackage.json'),
+        load('out/test_save_load_dates/datapackage.json'),
         printer()
     ).results()
 
@@ -1236,7 +1237,7 @@ def test_load_override_schema_and_fields():
         {'name': 'george', 'age': '17'},
         {'name': None, 'age': '22'},
     ]]
-      
+
 def test_delete_fields_regex():
     from dataflows import load, delete_fields
     flow = Flow(
@@ -1271,3 +1272,29 @@ def test_join_full_outer():
         {'id': 3, 'city': 'rome', 'population': None},
         {'id': 4, 'city': None, 'population': 3},
     ]]
+
+
+def test_load_duplicate_headers():
+    from dataflows import load
+    flow = Flow(
+        load('data/duplicate_headers.csv'),
+    )
+    with pytest.raises(ValueError) as excinfo:
+        flow.results()
+    assert 'duplicate headers' in str(excinfo.value)
+
+
+def test_load_duplicate_headers_with_deduplicate_headers_flag():
+    from dataflows import load
+    flow = Flow(
+        load('data/duplicate_headers.csv', deduplicate_headers=True),
+    )
+    data, package, stats = flow.results()
+    assert package.descriptor['resources'][0]['schema']['fields'] == [
+        {'name': 'header1', 'type': 'string', 'format': 'default'},
+        {'name': 'header2 (1)', 'type': 'string', 'format': 'default'},
+        {'name': 'header2 (2)', 'type': 'string', 'format': 'default'},
+    ]
+    assert data == [[
+        {'header1': 'value1', 'header2 (1)': 'value2', 'header2 (2)': 'value3'},
+    ]]
