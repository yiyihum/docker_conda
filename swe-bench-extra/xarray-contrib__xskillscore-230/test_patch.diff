diff --git a/.github/workflows/xskillscore_testing.yml b/.github/workflows/xskillscore_testing.yml
index 4ed6d27..0508ea9 100644
--- a/.github/workflows/xskillscore_testing.yml
+++ b/.github/workflows/xskillscore_testing.yml
@@ -60,7 +60,7 @@ jobs:
           channels: conda-forge
           mamba-version: '*'
           activate-environment: xskillscore-docs-notebooks
-          python-version: 3.6
+          python-version: 3.8
       - name: Set up conda environment
         run: |
           mamba env update -f ci/docs_notebooks.yml
diff --git a/xskillscore/tests/conftest.py b/xskillscore/tests/conftest.py
index 059f3e2..9998ae9 100644
--- a/xskillscore/tests/conftest.py
+++ b/xskillscore/tests/conftest.py
@@ -68,6 +68,13 @@ def b_nan(b):
     return b.copy().where(b < 0.5)
 
 
+# with zeros
+@pytest.fixture
+def a_with_zeros(a):
+    """Zeros"""
+    return a.copy().where(a < 0.5, 0)
+
+
 # dask
 @pytest.fixture
 def a_dask(a):
@@ -116,6 +123,12 @@ def b_1d_nan(a_1d_nan):
     return b
 
 
+@pytest.fixture
+def a_1d_with_zeros(a_with_zeros):
+    """Timeseries of a with zeros"""
+    return a_with_zeros.isel(lon=0, lat=0, drop=True)
+
+
 # weights
 @pytest.fixture
 def weights(a):
diff --git a/xskillscore/tests/test_metric_results_accurate.py b/xskillscore/tests/test_metric_results_accurate.py
index a00e3d4..54f1481 100644
--- a/xskillscore/tests/test_metric_results_accurate.py
+++ b/xskillscore/tests/test_metric_results_accurate.py
@@ -2,7 +2,12 @@ import numpy as np
 import pytest
 import sklearn.metrics
 from scipy.stats import pearsonr, spearmanr
-from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
+from sklearn.metrics import (
+    mean_absolute_error,
+    mean_absolute_percentage_error,
+    mean_squared_error,
+    r2_score,
+)
 
 import xskillscore as xs
 from xskillscore.core.deterministic import (
@@ -23,6 +28,11 @@ xs_skl_metrics = [
     (r2, r2_score),
     (mse, mean_squared_error),
     (mae, mean_absolute_error),
+    (mape, mean_absolute_percentage_error),
+]
+
+xs_skl_metrics_with_zeros = [
+    (mape, mean_absolute_percentage_error),
 ]
 
 xs_scipy_metrics = [
@@ -34,7 +44,6 @@ xs_scipy_metrics = [
 
 
 xs_np_metrics = [
-    (mape, lambda x, y: np.mean(np.abs((x - y) / x))),
     (me, lambda x, y: np.mean(x - y)),
     (smape, lambda x, y: 1 / len(x) * np.sum(np.abs(y - x) / (np.abs(x) + np.abs(y)))),
 ]
@@ -69,6 +78,15 @@ def test_xs_same_as_skl_same_name(a_1d, b_1d, request):
     assert np.allclose(actual, expected)
 
 
+@pytest.mark.parametrize("xs_skl_metrics", xs_skl_metrics_with_zeros)
+def test_xs_same_as_skl_with_zeros(a_1d_with_zeros, b_1d, xs_skl_metrics):
+    """Tests xskillscore metric is same as scikit-learn metric."""
+    xs_metric, skl_metric = xs_skl_metrics
+    actual = xs_metric(a_1d_with_zeros, b_1d, "time")
+    expected = skl_metric(a_1d_with_zeros, b_1d)
+    assert np.allclose(actual, expected)
+
+
 @pytest.mark.parametrize("xs_scipy_metrics", xs_scipy_metrics)
 def test_xs_same_as_scipy(a_1d, b_1d, xs_scipy_metrics):
     """Tests xskillscore metric is same as scipy metric."""
