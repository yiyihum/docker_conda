diff --git a/pyupgrade/_main.py b/pyupgrade/_main.py
index 5e1a424..bf75a62 100644
--- a/pyupgrade/_main.py
+++ b/pyupgrade/_main.py
@@ -390,6 +390,7 @@ def _build_import_removals() -> Dict[Version, Dict[str, Tuple[str, ...]]]:
         ((3, 7), ('generator_stop',)),
         ((3, 8), ()),
         ((3, 9), ()),
+        ((3, 10), ()),
     )
 
     prev: Tuple[str, ...] = ()
@@ -874,6 +875,10 @@ def main(argv: Optional[Sequence[str]] = None) -> int:
         '--py39-plus',
         action='store_const', dest='min_version', const=(3, 9),
     )
+    parser.add_argument(
+        '--py310-plus',
+        action='store_const', dest='min_version', const=(3, 10),
+    )
     args = parser.parse_args(argv)
 
     ret = 0
diff --git a/pyupgrade/_plugins/typing_pep604.py b/pyupgrade/_plugins/typing_pep604.py
new file mode 100644
index 0000000..6dfc573
--- /dev/null
+++ b/pyupgrade/_plugins/typing_pep604.py
@@ -0,0 +1,126 @@
+import ast
+import functools
+import sys
+from typing import Iterable
+from typing import List
+from typing import Tuple
+
+from tokenize_rt import Offset
+from tokenize_rt import Token
+
+from pyupgrade._ast_helpers import ast_to_offset
+from pyupgrade._ast_helpers import is_name_attr
+from pyupgrade._data import register
+from pyupgrade._data import State
+from pyupgrade._data import TokenFunc
+from pyupgrade._token_helpers import CLOSING
+from pyupgrade._token_helpers import find_closing_bracket
+from pyupgrade._token_helpers import find_token
+from pyupgrade._token_helpers import OPENING
+
+
+def _fix_optional(i: int, tokens: List[Token]) -> None:
+    j = find_token(tokens, i, '[')
+    k = find_closing_bracket(tokens, j)
+    if tokens[j].line == tokens[k].line:
+        tokens[k] = Token('CODE', ' | None')
+        del tokens[i:j + 1]
+    else:
+        tokens[j] = tokens[j]._replace(src='(')
+        tokens[k] = tokens[k]._replace(src=')')
+        tokens[i:j] = [Token('CODE', 'None | ')]
+
+
+def _fix_union(
+        i: int,
+        tokens: List[Token],
+        *,
+        arg: ast.expr,
+        arg_count: int,
+) -> None:
+    arg_offset = ast_to_offset(arg)
+    j = find_token(tokens, i, '[')
+    to_delete = []
+    commas: List[int] = []
+
+    arg_depth = -1
+    depth = 1
+    k = j + 1
+    while depth:
+        if tokens[k].src in OPENING:
+            if arg_depth == -1:
+                to_delete.append(k)
+            depth += 1
+        elif tokens[k].src in CLOSING:
+            depth -= 1
+            if 0 < depth < arg_depth:
+                to_delete.append(k)
+        elif tokens[k].offset == arg_offset:
+            arg_depth = depth
+        elif depth == arg_depth and tokens[k].src == ',':
+            if len(commas) >= arg_count - 1:
+                to_delete.append(k)
+            else:
+                commas.append(k)
+
+        k += 1
+    k -= 1
+
+    if tokens[j].line == tokens[k].line:
+        del tokens[k]
+        for comma in commas:
+            tokens[comma] = Token('CODE', ' |')
+        for paren in reversed(to_delete):
+            del tokens[paren]
+        del tokens[i:j + 1]
+    else:
+        tokens[j] = tokens[j]._replace(src='(')
+        tokens[k] = tokens[k]._replace(src=')')
+
+        for comma in commas:
+            tokens[comma] = Token('CODE', ' |')
+        for paren in reversed(to_delete):
+            del tokens[paren]
+        del tokens[i:j]
+
+
+def _supported_version(state: State) -> bool:
+    return (
+        state.in_annotation and (
+            state.settings.min_version >= (3, 10) or
+            'annotations' in state.from_imports['__future__']
+        )
+    )
+
+
+@register(ast.Subscript)
+def visit_Subscript(
+        state: State,
+        node: ast.Subscript,
+        parent: ast.AST,
+) -> Iterable[Tuple[Offset, TokenFunc]]:
+    if not _supported_version(state):
+        return
+
+    if is_name_attr(node.value, state.from_imports, 'typing', ('Optional',)):
+        yield ast_to_offset(node), _fix_optional
+    elif is_name_attr(node.value, state.from_imports, 'typing', ('Union',)):
+        if sys.version_info >= (3, 9):  # pragma: no cover (py39+)
+            node_slice: ast.expr = node.slice
+        elif isinstance(node.slice, ast.Index):  # pragma: no cover (<py39)
+            node_slice = node.slice.value
+        else:  # pragma: no cover (<py39)
+            return  # unexpected slice type
+
+        if isinstance(node_slice, ast.Tuple):
+            if node_slice.elts:
+                arg = node_slice.elts[0]
+                arg_count = len(node_slice.elts)
+            else:
+                return  # empty Union
+        else:
+            arg = node_slice
+            arg_count = 1
+
+        func = functools.partial(_fix_union, arg=arg, arg_count=arg_count)
+        yield ast_to_offset(node), func
diff --git a/pyupgrade/_token_helpers.py b/pyupgrade/_token_helpers.py
index 3aa9e05..0ab815c 100644
--- a/pyupgrade/_token_helpers.py
+++ b/pyupgrade/_token_helpers.py
@@ -425,7 +425,8 @@ def find_and_replace_call(
 
 
 def replace_name(i: int, tokens: List[Token], *, name: str, new: str) -> None:
-    new_token = Token('CODE', new)
+    # preserve token offset in case we need to match it later
+    new_token = tokens[i]._replace(name='CODE', src=new)
     j = i
     while tokens[j].src != name:
         # timid: if we see a parenthesis here, skip it
