diff --git a/avalanche/benchmarks/scenarios/__init__.py b/avalanche/benchmarks/scenarios/__init__.py
index f84a965c..83db8205 100644
--- a/avalanche/benchmarks/scenarios/__init__.py
+++ b/avalanche/benchmarks/scenarios/__init__.py
@@ -8,3 +8,4 @@ from .task_aware import *
 from .dataset_scenario import *
 from .exmodel_scenario import *
 from .online import *
+from .validation_scenario import *
diff --git a/avalanche/benchmarks/scenarios/dataset_scenario.py b/avalanche/benchmarks/scenarios/dataset_scenario.py
index df0848ef..cf5f7afa 100644
--- a/avalanche/benchmarks/scenarios/dataset_scenario.py
+++ b/avalanche/benchmarks/scenarios/dataset_scenario.py
@@ -14,7 +14,6 @@
 import random
 from avalanche.benchmarks.utils.data import AvalancheDataset
 import torch
-from itertools import tee
 from typing import (
     Callable,
     Generator,
@@ -253,94 +252,9 @@ class LazyTrainValSplitter:
             yield self.split_strategy(new_experience.dataset)
 
 
-def benchmark_with_validation_stream(
-    benchmark: CLScenario,
-    validation_size: Union[int, float] = 0.5,
-    shuffle: bool = False,
-    seed: Optional[int] = None,
-    split_strategy: Optional[
-        Callable[[AvalancheDataset], Tuple[AvalancheDataset, AvalancheDataset]]
-    ] = None,
-) -> CLScenario:
-    """Helper to obtain a benchmark with a validation stream.
-
-    This generator accepts an existing benchmark instance and returns a version
-    of it in which the train stream has been split into training and validation
-    streams.
-
-    Each train/validation experience will be by splitting the original training
-    experiences. Patterns selected for the validation experience will be removed
-    from the training experiences.
-
-    The default splitting strategy is a random split as implemented by `split_validation_random`.
-    If you want to use class balancing you can use `split_validation_class_balanced`, or
-    use a custom `split_strategy`, as shown in the following example::
-
-        validation_size = 0.2
-        foo = lambda exp: split_dataset_class_balanced(validation_size, exp)
-        bm = benchmark_with_validation_stream(bm, custom_split_strategy=foo)
-
-    :param benchmark: The benchmark to split.
-    :param validation_size: The size of the validation experience, as an int
-        or a float between 0 and 1. Ignored if `custom_split_strategy` is used.
-    :param shuffle: If True, patterns will be allocated to the validation
-        stream randomly. This will use the default PyTorch random number
-        generator at its current state. Defaults to False. Ignored if
-        `custom_split_strategy` is used. If False, the first instances will be
-        allocated to the training  dataset by leaving the last ones to the
-        validation dataset.
-    :param split_strategy: A function that implements a custom splitting
-        strategy. The function must accept an AvalancheDataset and return a tuple
-        containing the new train and validation dataset. By default, the splitting
-        strategy will split the data according to `validation_size` and `shuffle`).
-        A good starting to understand the mechanism is to look at the
-        implementation of the standard splitting function
-        :func:`random_validation_split_strategy`.
-
-    :return: A benchmark instance in which the validation stream has been added.
-    """
-
-    if split_strategy is None:
-        if seed is None:
-            seed = random.randint(0, 1000000)
-
-        # functools.partial is a more compact option
-        # However, MyPy does not understand what a partial is -_-
-        def random_validation_split_strategy_wrapper(data):
-            return split_validation_random(validation_size, shuffle, seed, data)
-
-        split_strategy = random_validation_split_strategy_wrapper
-    else:
-        split_strategy = split_strategy
-
-    stream = benchmark.streams["train"]
-    if isinstance(stream, EagerCLStream):  # eager split
-        train_exps, valid_exps = [], []
-
-        exp: DatasetExperience
-        for exp in stream:
-            train_data, valid_data = split_strategy(exp.dataset)
-            train_exps.append(DatasetExperience(dataset=train_data))
-            valid_exps.append(DatasetExperience(dataset=valid_data))
-    else:  # Lazy splitting (based on a generator)
-        split_generator = LazyTrainValSplitter(split_strategy, stream)
-        train_exps = (DatasetExperience(dataset=a) for a, _ in split_generator)
-        valid_exps = (DatasetExperience(dataset=b) for _, b in split_generator)
-
-    train_stream = make_stream(name="train", exps=train_exps)
-    valid_stream = make_stream(name="valid", exps=valid_exps)
-    other_streams = benchmark.streams
-
-    del other_streams["train"]
-    return CLScenario(
-        streams=[train_stream, valid_stream] + list(other_streams.values())
-    )
-
-
 __all__ = [
     "_split_dataset_by_attribute",
     "benchmark_from_datasets",
     "DatasetExperience",
     "split_validation_random",
-    "benchmark_with_validation_stream",
 ]
diff --git a/avalanche/benchmarks/scenarios/supervised.py b/avalanche/benchmarks/scenarios/supervised.py
index 66c0ecc8..8e46446d 100644
--- a/avalanche/benchmarks/scenarios/supervised.py
+++ b/avalanche/benchmarks/scenarios/supervised.py
@@ -26,12 +26,11 @@ import torch
 from avalanche.benchmarks.utils.classification_dataset import (
     ClassificationDataset,
     _as_taskaware_supervised_classification_dataset,
-    TaskAwareSupervisedClassificationDataset,
 )
 from avalanche.benchmarks.utils.data import AvalancheDataset
 from avalanche.benchmarks.utils.data_attribute import DataAttribute
 from .dataset_scenario import _split_dataset_by_attribute, DatasetExperience
-from .. import CLScenario, CLStream, EagerCLStream
+from .generic_scenario import CLScenario, CLStream, EagerCLStream
 
 
 def class_incremental_benchmark(
diff --git a/avalanche/models/dynamic_modules.py b/avalanche/models/dynamic_modules.py
index 1cf58098..3086681f 100644
--- a/avalanche/models/dynamic_modules.py
+++ b/avalanche/models/dynamic_modules.py
@@ -246,7 +246,7 @@ class IncrementalClassifier(DynamicModule):
                 self.active_units[: old_act_units.shape[0]] = old_act_units
             # update with new active classes
             if self.training:
-                self.active_units[curr_classes] = 1
+                self.active_units[list(curr_classes)] = 1
 
         # update classifier weights
         if old_nclasses == new_nclasses:

