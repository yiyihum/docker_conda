diff --git a/tests/_sanity/test_sanity.py b/tests/_sanity/test_sanity.py
index 9ce86e9..3cad182 100644
--- a/tests/_sanity/test_sanity.py
+++ b/tests/_sanity/test_sanity.py
@@ -27,13 +27,13 @@ def bad_luhn_fake_account_num_delims():
 
 
 def test_luhn_with_passing_account_num(good_luhn_fake_account_num):
-    rv = luhn(good_luhn_fake_account_num)
+    rv = luhn(good_luhn_fake_account_num, "utf-8")
     assert rv == True, "Should have returned True"
     assert isinstance(rv, bool), "Return value should be a bool"
 
 
 def test_luhn_with_failing_account_num(bad_luhn_fake_account_num):
-    rv = luhn(bad_luhn_fake_account_num)
+    rv = luhn(bad_luhn_fake_account_num, "utf-8")
     assert rv == False, "Should have returned False"
     assert isinstance(rv, bool), "Return value should be a bool"
 
@@ -41,13 +41,13 @@ def test_luhn_with_failing_account_num(bad_luhn_fake_account_num):
 def test_luhn_for_value_error():
     non_int = "123abc"
     with pytest.raises(ValueError) as e_info:
-        _ = luhn(non_int)
+        _ = luhn(non_int, "utf-8")
 
 
 def test_luhn_for_value_error_with_delimeters():
     non_int_with_delims = "123-abc"
     with pytest.raises(ValueError) as e_info:
-        _ = luhn(non_int_with_delims)
+        _ = luhn(non_int_with_delims, "utf-8")
 
 
 # Sanity check function tests
@@ -55,7 +55,7 @@ def test_luhn_for_value_error_with_delimeters():
 
 @pytest.fixture(scope="module")
 def always_true_algorithm_stub():
-    def stub_func(not_used):
+    def stub_func(not_used, also_not_used):
         return True
 
     return stub_func
@@ -63,7 +63,7 @@ def always_true_algorithm_stub():
 
 @pytest.fixture(scope="module")
 def always_false_algorithm_stub():
-    def stub_func(not_used):
+    def stub_func(not_used, also_not_used):
         return False
 
     return stub_func
@@ -76,7 +76,7 @@ def test_sanity_check_passes_sanity(always_true_algorithm_stub):
     name = "always_true"
     test_sanity_map = {name: always_true_algorithm_stub}
     data = "placeholder"
-    rv = sanity_check(name, data, sanity_map=test_sanity_map)
+    rv = sanity_check(name, data, encoding="utf-8", sanity_map=test_sanity_map)
     assert rv == True, "Should have returned True"
     assert isinstance(rv, bool), "Return value should have been a bool"
 
@@ -85,7 +85,7 @@ def test_sanity_check_fails_sanity(always_false_algorithm_stub):
     name = "always_false"
     test_sanity_map = {name: always_false_algorithm_stub}
     data = "placeholder"
-    rv = sanity_check(name, data, sanity_map=test_sanity_map)
+    rv = sanity_check(name, data, encoding="utf-8", sanity_map=test_sanity_map)
     assert rv == False, "Should have returned False"
     assert isinstance(rv, bool), "Return value should have been a bool"
 
@@ -95,4 +95,4 @@ def test_sanity_check_algorithm_name_doesnt_exist():
     test_sanity_map = {"not_real": "me_either"}
     data = "placeholder"
     with pytest.raises(ValueError) as e_info:
-        sanity_check(name, data, sanity_map=test_sanity_map)
+        sanity_check(name, data, encoding="utf-8", sanity_map=test_sanity_map)
diff --git a/tests/core/test_core.py b/tests/core/test_core.py
index babadcf..60b6764 100644
--- a/tests/core/test_core.py
+++ b/tests/core/test_core.py
@@ -16,8 +16,8 @@ def test_gzipped_file_check_return_true():
     def opener_stub_raise_error(x, y):
         class FileHandlerStub:
             @staticmethod
-            def readline():
-                raise UnicodeDecodeError("fake", b"o", 1, 2, "fake")
+            def read(not_used):
+                return b"\x1f\x8b"
 
         yield FileHandlerStub()
 
@@ -29,8 +29,8 @@ def test_gzipped_file_check_return_false():
     def opener_stub_no_error(x, y):
         class FileHandlerStub:
             @staticmethod
-            def readline():
-                return ""
+            def read(not_used):
+                return b"NOPE"
 
         yield FileHandlerStub()
 
@@ -44,7 +44,7 @@ def test_tokenize_not_show_matches():
 
 def test_tokenize_return_clear_text():
 
-    assert tokenize("hello", "XXX", 0, tokenize=False, show_matches=True) == "hello"
+    assert tokenize(b"hello", b"XXX", 0, tokenize=False, show_matches=True) == b"hello"
 
 
 def test_tokenize_runs_tokenization_function():
@@ -52,20 +52,11 @@ def test_tokenize_runs_tokenization_function():
         return "stub was called"
 
     assert (
-        tokenize("hello", "XXX", 0, show_matches=True, tokenize_func=stub_func)
-        == "stub was called"
+        tokenize(b"hello", b"XXX", 0, show_matches=True, tokenize_func=stub_func)
+        == b"stub was called"
     )
 
 
-def test_tokenize_for_byte_return():
-    def stub_func(arg1, arg2, arg3):
-        return (arg1, arg2, arg3)
-
-    assert tokenize(
-        b"hello", b"XXX", 0, show_matches=True, tokenize_func=stub_func
-    ) == ("hello", "XXX", 0)
-
-
 def test_get_tokenized_string_normal():
 
     text = "howdy"
@@ -86,20 +77,20 @@ def test_get_tokenized_string_mask_too_long():
 
 def test_byte_code_to_string_no_byte_string():
 
-    fake_byte_code = "bhello"
+    fake_byte_code = b"bhello"
 
-    assert _byte_code_to_string(fake_byte_code) == fake_byte_code
+    assert _byte_code_to_string(fake_byte_code, _encoding="utf-8") == fake_byte_code
 
 
 def test_byte_code_to_string_start_of_header():
 
-    byte_code = "b1"
+    byte_code = b"b1"
 
-    assert _byte_code_to_string(byte_code) == "\x01"
+    assert _byte_code_to_string(byte_code, _encoding="utf-8") == b"\x01"
 
 
 def test_sanity_for_failed_sanity_check():
-    def stub_func(a, b):
+    def stub_func(a, b, encoding):
         return False
 
     class StubFilter:
@@ -111,7 +102,7 @@ def test_sanity_for_failed_sanity_check():
 
 
 def test_sanity_for_passed_sanity_checks():
-    def stub_func(a, b):
+    def stub_func(a, b, encoding):
         return True
 
     class StubFilter:
