diff --git a/feature_engine/encoding/mean_encoding.py b/feature_engine/encoding/mean_encoding.py
index 0816dee..40b9ad9 100644
--- a/feature_engine/encoding/mean_encoding.py
+++ b/feature_engine/encoding/mean_encoding.py
@@ -141,13 +141,10 @@ class MeanEncoder(CategoricalInitExpandedMixin, CategoricalMethodsMixin):
         self._fit(X)
         self._get_feature_names_in(X)
 
-        temp = pd.concat([X, y], axis=1)
-        temp.columns = list(X.columns) + ["target"]
-
         self.encoder_dict_ = {}
 
         for var in self.variables_:
-            self.encoder_dict_[var] = temp.groupby(var)["target"].mean().to_dict()
+            self.encoder_dict_[var] = y.groupby(X[var]).mean().to_dict()
 
         self._check_encoding_dictionary()
 
diff --git a/feature_engine/encoding/ordinal.py b/feature_engine/encoding/ordinal.py
index 4d6f6d3..5c9069f 100644
--- a/feature_engine/encoding/ordinal.py
+++ b/feature_engine/encoding/ordinal.py
@@ -160,22 +160,14 @@ class OrdinalEncoder(CategoricalInitExpandedMixin, CategoricalMethodsMixin):
         self._fit(X)
         self._get_feature_names_in(X)
 
-        if self.encoding_method == "ordered":
-            temp = pd.concat([X, y], axis=1)
-            temp.columns = list(X.columns) + ["target"]
-
         # find mappings
         self.encoder_dict_ = {}
 
         for var in self.variables_:
 
             if self.encoding_method == "ordered":
-                t = (
-                    temp.groupby([var])["target"]
-                    .mean()
-                    .sort_values(ascending=True)
-                    .index
-                )
+                t = y.groupby(X[var]).mean()  # type: ignore
+                t = t.sort_values(ascending=True).index
 
             elif self.encoding_method == "arbitrary":
                 t = X[var].unique()
diff --git a/feature_engine/encoding/woe.py b/feature_engine/encoding/woe.py
index f151f50..11a9098 100644
--- a/feature_engine/encoding/woe.py
+++ b/feature_engine/encoding/woe.py
@@ -151,40 +151,33 @@ class WoEEncoder(CategoricalInitExpandedMixin, CategoricalMethodsMixin):
                 "used has more than 2 unique values."
             )
 
-        self._fit(X)
-        self._get_feature_names_in(X)
-
-        temp = pd.concat([X, y], axis=1)
-        temp.columns = list(X.columns) + ["target"]
-
         # if target does not have values 0 and 1, we need to remap, to be able to
         # compute the averages.
-        if any(x for x in y.unique() if x not in [0, 1]):
-            temp["target"] = np.where(temp["target"] == y.unique()[0], 0, 1)
+        if y.min() != 0 or y.max() != 1:
+            y = pd.Series(np.where(y == y.min(), 0, 1))
+
+        self._fit(X)
+        self._get_feature_names_in(X)
 
         self.encoder_dict_ = {}
 
-        total_pos = temp["target"].sum()
-        total_neg = len(temp) - total_pos
-        temp["non_target"] = np.where(temp["target"] == 1, 0, 1)
+        total_pos = y.sum()
+        inverse_y = y.ne(1).copy()
+        total_neg = inverse_y.sum()
 
         for var in self.variables_:
-            pos = temp.groupby([var])["target"].sum() / total_pos
-            neg = temp.groupby([var])["non_target"].sum() / total_neg
+            pos = y.groupby(X[var]).sum() / total_pos
+            neg = inverse_y.groupby(X[var]).sum() / total_neg
 
-            t = pd.concat([pos, neg], axis=1)
-            t["woe"] = np.log(t["target"] / t["non_target"])
-
-            if (
-                not t.loc[t["target"] == 0, :].empty
-                or not t.loc[t["non_target"] == 0, :].empty
-            ):
+            if not (pos[:] == 0).sum() == 0 or not (neg[:] == 0).sum() == 0:
                 raise ValueError(
                     "The proportion of one of the classes for a category in "
                     "variable {} is zero, and log of zero is not defined".format(var)
                 )
 
-            self.encoder_dict_[var] = t["woe"].to_dict()
+            woe = np.log(pos / neg)
+
+            self.encoder_dict_[var] = woe.to_dict()
 
         self._check_encoding_dictionary()
 

