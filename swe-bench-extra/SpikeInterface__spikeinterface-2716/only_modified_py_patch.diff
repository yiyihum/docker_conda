diff --git a/examples/modules_gallery/core/plot_4_sorting_analyzer.py b/examples/modules_gallery/core/plot_4_sorting_analyzer.py
index b41109bf6..f38746bbb 100644
--- a/examples/modules_gallery/core/plot_4_sorting_analyzer.py
+++ b/examples/modules_gallery/core/plot_4_sorting_analyzer.py
@@ -72,7 +72,12 @@ print(analyzer)
 # when using format="binary_folder" or format="zarr"
 
 folder = "analyzer_folder"
-analyzer = create_sorting_analyzer(sorting=sorting, recording=recording, format="binary_folder", folder=folder)
+analyzer = create_sorting_analyzer(sorting=sorting,
+                                   recording=recording,
+                                   format="binary_folder",
+                                   return_scaled=True, # this is the default to attempt to return scaled
+                                   folder=folder
+                                   )
 print(analyzer)
 
 # then it can be loaded back
@@ -90,7 +95,7 @@ analyzer.compute(
     method="uniform",
     max_spikes_per_unit=500,
 )
-analyzer.compute("waveforms", ms_before=1.0, ms_after=2.0, return_scaled=True)
+analyzer.compute("waveforms", ms_before=1.0, ms_after=2.0)
 analyzer.compute("templates", operators=["average", "median", "std"])
 print(analyzer)
 
@@ -100,12 +105,12 @@ print(analyzer)
 # using parallel processing (recommended!). Like this
 
 analyzer.compute(
-    "waveforms", ms_before=1.0, ms_after=2.0, return_scaled=True, n_jobs=8, chunk_duration="1s", progress_bar=True
+    "waveforms", ms_before=1.0, ms_after=2.0, n_jobs=8, chunk_duration="1s", progress_bar=True
 )
 
 # which is equivalent to this:
 job_kwargs = dict(n_jobs=8, chunk_duration="1s", progress_bar=True)
-analyzer.compute("waveforms", ms_before=1.0, ms_after=2.0, return_scaled=True, **job_kwargs)
+analyzer.compute("waveforms", ms_before=1.0, ms_after=2.0, **job_kwargs)
 
 #################################################################################
 # Because certain extensions rely on others (e.g. we need waveforms to calculate
diff --git a/src/spikeinterface/core/template.py b/src/spikeinterface/core/template.py
index d85faa751..51688709b 100644
--- a/src/spikeinterface/core/template.py
+++ b/src/spikeinterface/core/template.py
@@ -108,6 +108,23 @@ class Templates:
                 if not self._are_passed_templates_sparse():
                     raise ValueError("Sparsity mask passed but the templates are not sparse")
 
+    def __repr__(self):
+        sampling_frequency_khz = self.sampling_frequency / 1000
+        repr_str = (
+            f"Templates: {self.num_units} units - {self.num_samples} samples - {self.num_channels} channels \n"
+            f"sampling_frequency={sampling_frequency_khz:.2f} kHz - "
+            f"ms_before={self.ms_before:.2f} ms - "
+            f"ms_after={self.ms_after:.2f} ms"
+        )
+
+        if self.probe is not None:
+            repr_str += f"\n{self.probe.__repr__()}"
+
+        if self.sparsity is not None:
+            repr_str += f"\n{self.sparsity.__repr__()}"
+
+        return repr_str
+
     def to_sparse(self, sparsity):
         # Turn a dense representation of templates into a sparse one, given some sparsity.
         # Note that nothing prevent Templates tobe empty after sparsification if the sparse mask have no channels for some units
diff --git a/src/spikeinterface/curation/remove_excess_spikes.py b/src/spikeinterface/curation/remove_excess_spikes.py
index b9f00212e..52653d684 100644
--- a/src/spikeinterface/curation/remove_excess_spikes.py
+++ b/src/spikeinterface/curation/remove_excess_spikes.py
@@ -79,8 +79,9 @@ class RemoveExcessSpikesSortingSegment(BaseSortingSegment):
     ) -> np.ndarray:
         spike_train = self._parent_segment.get_unit_spike_train(unit_id, start_frame=start_frame, end_frame=end_frame)
         max_spike = np.searchsorted(spike_train, self._num_samples, side="left")
+        min_spike = np.searchsorted(spike_train, 0, side="left")
 
-        return spike_train[:max_spike]
+        return spike_train[min_spike:max_spike]
 
 
 def remove_excess_spikes(sorting, recording):

