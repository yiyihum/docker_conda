diff --git a/cmdstanpy/cmdstan_args.py b/cmdstanpy/cmdstan_args.py
index 9bbec6c..2fc7526 100644
--- a/cmdstanpy/cmdstan_args.py
+++ b/cmdstanpy/cmdstan_args.py
@@ -396,7 +396,7 @@ class OptimizeArgs:
         history_size: Optional[int] = None,
     ) -> None:
 
-        self.algorithm = algorithm
+        self.algorithm = algorithm or ""
         self.init_alpha = init_alpha
         self.iter = iter
         self.save_iterations = save_iterations
@@ -414,10 +414,7 @@ class OptimizeArgs:
         """
         Check arguments correctness and consistency.
         """
-        if (
-            self.algorithm is not None
-            and self.algorithm not in self.OPTIMIZE_ALGOS
-        ):
+        if self.algorithm and self.algorithm not in self.OPTIMIZE_ALGOS:
             raise ValueError(
                 'Please specify optimizer algorithms as one of [{}]'.format(
                     ', '.join(self.OPTIMIZE_ALGOS)
@@ -425,9 +422,9 @@ class OptimizeArgs:
             )
 
         if self.init_alpha is not None:
-            if self.algorithm == 'Newton':
+            if self.algorithm.lower() not in {'lbfgs', 'bfgs'}:
                 raise ValueError(
-                    'init_alpha must not be set when algorithm is Newton'
+                    'init_alpha requires that algorithm be set to bfgs or lbfgs'
                 )
             if isinstance(self.init_alpha, float):
                 if self.init_alpha <= 0:
@@ -443,9 +440,9 @@ class OptimizeArgs:
                 raise ValueError('iter must be type of int')
 
         if self.tol_obj is not None:
-            if self.algorithm == 'Newton':
+            if self.algorithm.lower() not in {'lbfgs', 'bfgs'}:
                 raise ValueError(
-                    'tol_obj must not be set when algorithm is Newton'
+                    'tol_obj requires that algorithm be set to bfgs or lbfgs'
                 )
             if isinstance(self.tol_obj, float):
                 if self.tol_obj <= 0:
@@ -454,9 +451,10 @@ class OptimizeArgs:
                 raise ValueError('tol_obj must be type of float')
 
         if self.tol_rel_obj is not None:
-            if self.algorithm == 'Newton':
+            if self.algorithm.lower() not in {'lbfgs', 'bfgs'}:
                 raise ValueError(
-                    'tol_rel_obj must not be set when algorithm is Newton'
+                    'tol_rel_obj requires that algorithm be set to bfgs'
+                    ' or lbfgs'
                 )
             if isinstance(self.tol_rel_obj, float):
                 if self.tol_rel_obj <= 0:
@@ -465,9 +463,9 @@ class OptimizeArgs:
                 raise ValueError('tol_rel_obj must be type of float')
 
         if self.tol_grad is not None:
-            if self.algorithm == 'Newton':
+            if self.algorithm.lower() not in {'lbfgs', 'bfgs'}:
                 raise ValueError(
-                    'tol_grad must not be set when algorithm is Newton'
+                    'tol_grad requires that algorithm be set to bfgs or lbfgs'
                 )
             if isinstance(self.tol_grad, float):
                 if self.tol_grad <= 0:
@@ -476,9 +474,10 @@ class OptimizeArgs:
                 raise ValueError('tol_grad must be type of float')
 
         if self.tol_rel_grad is not None:
-            if self.algorithm == 'Newton':
+            if self.algorithm.lower() not in {'lbfgs', 'bfgs'}:
                 raise ValueError(
-                    'tol_rel_grad must not be set when algorithm is Newton'
+                    'tol_rel_grad requires that algorithm be set to bfgs'
+                    ' or lbfgs'
                 )
             if isinstance(self.tol_rel_grad, float):
                 if self.tol_rel_grad <= 0:
@@ -487,9 +486,9 @@ class OptimizeArgs:
                 raise ValueError('tol_rel_grad must be type of float')
 
         if self.tol_param is not None:
-            if self.algorithm == 'Newton':
+            if self.algorithm.lower() not in {'lbfgs', 'bfgs'}:
                 raise ValueError(
-                    'tol_param must not be set when algorithm is Newton'
+                    'tol_param requires that algorithm be set to bfgs or lbfgs'
                 )
             if isinstance(self.tol_param, float):
                 if self.tol_param <= 0:
@@ -498,10 +497,9 @@ class OptimizeArgs:
                 raise ValueError('tol_param must be type of float')
 
         if self.history_size is not None:
-            if self.algorithm == 'Newton' or self.algorithm == 'BFGS':
+            if self.algorithm.lower() != 'lbfgs':
                 raise ValueError(
-                    'history_size must not be set when algorithm is '
-                    'Newton or BFGS'
+                    'history_size requires that algorithm be set to lbfgs'
                 )
             if isinstance(self.history_size, int):
                 if self.history_size < 0:
diff --git a/cmdstanpy/model.py b/cmdstanpy/model.py
index 723bc7e..81e8464 100644
--- a/cmdstanpy/model.py
+++ b/cmdstanpy/model.py
@@ -210,12 +210,6 @@ class CmdStanModel:
 
         if compile and self._exe_file is None:
             self.compile(force=str(compile).lower() == 'force')
-            if self._exe_file is None:
-                raise ValueError(
-                    'Unable to compile Stan model file: {}.'.format(
-                        self._stan_file
-                    )
-                )
 
     def __repr__(self) -> str:
         repr = 'CmdStanModel: name={}'.format(self._name)
@@ -531,45 +525,26 @@ class CmdStanModel:
                 get_logger().info(
                     'compiled model executable: %s', self._exe_file
                 )
-            if compilation_failed or 'Warning' in console:
+            if 'Warning' in console:
                 lines = console.split('\n')
                 warnings = [x for x in lines if x.startswith('Warning')]
-                syntax_errors = [
-                    x for x in lines if x.startswith('Syntax error')
-                ]
-                semantic_errors = [
-                    x for x in lines if x.startswith('Semantic error')
-                ]
-                exceptions = [
-                    x
-                    for x in lines
-                    if 'Uncaught exception' in x or 'fatal error' in x
-                ]
-                if (
-                    len(syntax_errors) > 0
-                    or len(semantic_errors) > 0
-                    or len(exceptions) > 0
-                ):
-                    get_logger().error('Stan program failed to compile:')
-                    get_logger().warning(console)
-                elif len(warnings) > 0:
-                    get_logger().warning(
-                        'Stan compiler has produced %d warnings:',
-                        len(warnings),
-                    )
-                    get_logger().warning(console)
-                elif (
-                    'PCH file' in console
-                    or 'model_header.hpp.gch' in console
-                    or 'precompiled header' in console
-                ):
+                get_logger().warning(
+                    'Stan compiler has produced %d warnings:',
+                    len(warnings),
+                )
+                get_logger().warning(console)
+            if compilation_failed:
+                if 'PCH' in console or 'precompiled header' in console:
                     get_logger().warning(
                         "CmdStan's precompiled header (PCH) files "
                         "may need to be rebuilt."
-                        "If your model failed to compile please run "
-                        "cmdstanpy.rebuild_cmdstan().\nIf the "
-                        "issue persists please open a bug report"
+                        "Please run cmdstanpy.rebuild_cmdstan().\n"
+                        "If the issue persists please open a bug report"
                     )
+                raise ValueError(
+                    f"Failed to compile Stan model '{self._stan_file}'. "
+                    f"Console:\n{console}"
+                )
 
     def optimize(
         self,
@@ -726,7 +701,9 @@ class CmdStanModel:
             self._run_cmdstan(runset, dummy_chain_id, show_console=show_console)
 
         if not runset._check_retcodes():
-            msg = 'Error during optimization: {}'.format(runset.get_err_msgs())
+            msg = "Error during optimization! Command '{}' failed: {}".format(
+                ' '.join(runset.cmd(0)), runset.get_err_msgs()
+            )
             if 'Line search failed' in msg and not require_converged:
                 get_logger().warning(msg)
             else:

