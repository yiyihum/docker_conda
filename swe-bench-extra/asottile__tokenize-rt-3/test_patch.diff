diff --git a/tests/tokenize_rt_test.py b/tests/tokenize_rt_test.py
index d01f0fb..6977e5f 100644
--- a/tests/tokenize_rt_test.py
+++ b/tests/tokenize_rt_test.py
@@ -5,6 +5,7 @@ import io
 
 import pytest
 
+from tokenize_rt import ESCAPED_NL
 from tokenize_rt import main
 from tokenize_rt import src_to_tokens
 from tokenize_rt import Token
@@ -26,6 +27,41 @@ def test_src_to_tokens_simple():
     ]
 
 
+def test_src_to_tokens_escaped_nl():
+    src = (
+        'x = \\\n'
+        '    5'
+    )
+    ret = src_to_tokens(src)
+    assert ret == [
+        Token('NAME', 'x', line=1, utf8_byte_offset=0),
+        Token(UNIMPORTANT_WS, ' ', line=None, utf8_byte_offset=None),
+        Token('OP', '=', line=1, utf8_byte_offset=2),
+        Token(UNIMPORTANT_WS, ' ', line=None, utf8_byte_offset=None),
+        Token(ESCAPED_NL, '\\\n', line=None, utf8_byte_offset=None),
+        Token(UNIMPORTANT_WS, '    ', line=None, utf8_byte_offset=None),
+        Token('NUMBER', '5', line=2, utf8_byte_offset=4),
+        Token('ENDMARKER', '', line=3, utf8_byte_offset=0),
+    ]
+
+
+def test_src_to_tokens_escaped_nl_no_left_ws():
+    src = (
+        'x =\\\n'
+        '    5'
+    )
+    ret = src_to_tokens(src)
+    assert ret == [
+        Token('NAME', 'x', line=1, utf8_byte_offset=0),
+        Token(UNIMPORTANT_WS, ' ', line=None, utf8_byte_offset=None),
+        Token('OP', '=', line=1, utf8_byte_offset=2),
+        Token(ESCAPED_NL, '\\\n', line=None, utf8_byte_offset=None),
+        Token(UNIMPORTANT_WS, '    ', line=None, utf8_byte_offset=None),
+        Token('NUMBER', '5', line=2, utf8_byte_offset=4),
+        Token('ENDMARKER', '', line=3, utf8_byte_offset=0),
+    ]
+
+
 @pytest.mark.parametrize(
     'filename',
     (
