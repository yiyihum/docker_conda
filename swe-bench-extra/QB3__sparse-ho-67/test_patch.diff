diff --git a/sparse_ho/tests/test_grad_search.py b/sparse_ho/tests/test_grad_search.py
index 492c04b..a8537f7 100644
--- a/sparse_ho/tests/test_grad_search.py
+++ b/sparse_ho/tests/test_grad_search.py
@@ -10,7 +10,7 @@ from sparse_ho.models import Lasso
 from sparse_ho import Forward
 from sparse_ho import ImplicitForward
 from sparse_ho import Implicit
-from sparse_ho.criterion import HeldOutMSE, SmoothedSURE
+from sparse_ho.criterion import HeldOutMSE, FiniteDiffMonteCarloSure
 from sparse_ho.ho import grad_search
 from sparse_ho.optimizers import LineSearch
 
@@ -64,7 +64,7 @@ def test_grad_search(model, crit):
         criterion = HeldOutMSE(idx_train, idx_val)
     else:
         n_outer = 2
-        criterion = SmoothedSURE(sigma_star)
+        criterion = FiniteDiffMonteCarloSure(sigma_star)
     # TODO MM@QBE if else scheme surprising
 
     criterion = HeldOutMSE(idx_train, idx_val)
diff --git a/sparse_ho/tests/test_grid_search.py b/sparse_ho/tests/test_grid_search.py
index 2e1d6c6..4df0ff7 100644
--- a/sparse_ho/tests/test_grid_search.py
+++ b/sparse_ho/tests/test_grid_search.py
@@ -6,7 +6,7 @@ from sparse_ho.utils import Monitor
 from sparse_ho.datasets import get_synt_data
 from sparse_ho.models import Lasso
 from sparse_ho import Forward
-from sparse_ho.criterion import HeldOutMSE, SmoothedSURE
+from sparse_ho.criterion import HeldOutMSE, FiniteDiffMonteCarloSure
 from sparse_ho.grid_search import grid_search
 
 
@@ -69,7 +69,7 @@ def test_grid_search():
     monitor_grid = Monitor()
     model = Lasso(estimator=estimator)
 
-    criterion = SmoothedSURE(sigma=sigma_star)
+    criterion = FiniteDiffMonteCarloSure(sigma=sigma_star)
     algo = Forward()
     log_alpha_opt_grid, _ = grid_search(
         algo, criterion, model, X, y, log_alpha_min, log_alpha_max,
@@ -77,7 +77,7 @@ def test_grid_search():
         tol=1e-5, samp="grid")
 
     monitor_random = Monitor()
-    criterion = SmoothedSURE(sigma=sigma_star)
+    criterion = FiniteDiffMonteCarloSure(sigma=sigma_star)
     algo = Forward()
     log_alpha_opt_random, _ = grid_search(
         algo, criterion, model, X, y, log_alpha_min, log_alpha_max,
diff --git a/sparse_ho/tests/test_lasso.py b/sparse_ho/tests/test_lasso.py
index 8852e2d..8553435 100644
--- a/sparse_ho/tests/test_lasso.py
+++ b/sparse_ho/tests/test_lasso.py
@@ -13,7 +13,7 @@ from sparse_ho import Forward
 from sparse_ho import ImplicitForward
 from sparse_ho import Implicit
 from sparse_ho import Backward
-from sparse_ho.criterion import HeldOutMSE, SmoothedSURE
+from sparse_ho.criterion import HeldOutMSE, FiniteDiffMonteCarloSure
 
 n_samples = 100
 n_features = 100
@@ -160,22 +160,22 @@ def test_val_grad():
     for key in models.keys():
         log_alpha = dict_log_alpha[key]
         model = models[key]
-        criterion = SmoothedSURE(sigma_star)
+        criterion = FiniteDiffMonteCarloSure(sigma_star)
         algo = Forward()
         val_fwd, grad_fwd = criterion.get_val_grad(
             model, X, y, log_alpha, algo.get_beta_jac_v, tol=tol)
 
-        criterion = SmoothedSURE(sigma_star)
+        criterion = FiniteDiffMonteCarloSure(sigma_star)
         algo = ImplicitForward(tol_jac=1e-8, n_iter_jac=5000)
         val_imp_fwd, grad_imp_fwd = criterion.get_val_grad(
             model, X, y, log_alpha, algo.get_beta_jac_v, tol=tol)
 
-        criterion = SmoothedSURE(sigma_star)
+        criterion = FiniteDiffMonteCarloSure(sigma_star)
         algo = Implicit(criterion)
         val_imp, grad_imp = criterion.get_val_grad(
             model, X, y, log_alpha, algo.get_beta_jac_v, tol=tol)
 
-        criterion = SmoothedSURE(sigma_star)
+        criterion = FiniteDiffMonteCarloSure(sigma_star)
         algo = Backward()
         val_bwd, grad_bwd = criterion.get_val_grad(
             model, X, y, log_alpha, algo.get_beta_jac_v, tol=tol)
