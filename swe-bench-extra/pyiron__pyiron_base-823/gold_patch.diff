diff --git a/pyiron_base/interfaces/has_hdf.py b/pyiron_base/interfaces/has_hdf.py
index 94e7b268..656d003c 100644
--- a/pyiron_base/interfaces/has_hdf.py
+++ b/pyiron_base/interfaces/has_hdf.py
@@ -23,6 +23,8 @@ class _WithHDF:
     __slots__ = ("_hdf", "_group_name")
 
     def __init__(self, hdf, group_name=None):
+        if group_name in hdf.list_nodes():
+            raise ValueError(f"{group_name} is a node and not a group!")
         self._hdf = hdf
         self._group_name = group_name
 
@@ -209,7 +211,10 @@ class HasHDF(ABC):
             group_name if group_name is not None else self._get_hdf_group_name()
         )
         with _WithHDF(hdf, group_name) as hdf:
-            if len(hdf.list_dirs()) > 0 and group_name is None:
+            if (
+                group_name is None
+                and (len(hdf.list_nodes()) > 0 or len(hdf.list_dirs())) > 0
+            ):
                 raise ValueError("HDF group must be empty when group_name is not set.")
             self._to_hdf(hdf)
             self._store_type_to_hdf(hdf)
diff --git a/pyiron_base/interfaces/object.py b/pyiron_base/interfaces/object.py
index 078b66c2..5f48e42e 100644
--- a/pyiron_base/interfaces/object.py
+++ b/pyiron_base/interfaces/object.py
@@ -27,15 +27,27 @@ __date__ = "Mar 23, 2021"
 class HasStorage(HasHDF, ABC):
     """
     A base class for objects that use HDF5 data serialization via the `DataContainer` class.
+
+    Unless you know what you are doing sub classes should pass the `group_name` argument to :meth:`~.__init__` or
+    override :meth:`~.get_hdf_group_name()` to force a default name for the HDF group the object should write itself to.
     """
 
-    def __init__(self, *args, **kwargs):
+    def __init__(self, *args, group_name=None, **kwargs):
+        """
+
+        Args:
+            group_name (str): default name of the HDF group where the whole object should be written to.
+        """
         self._storage = DataContainer(table_name="storage")
+        self._group_name = group_name
 
     @property
     def storage(self) -> DataContainer:
         return self._storage
 
+    def _get_hdf_group_name(self) -> str:
+        return self._group_name
+
     def _to_hdf(self, hdf: ProjectHDFio):
         self.storage.to_hdf(hdf=hdf)
 
diff --git a/pyiron_base/jobs/job/template.py b/pyiron_base/jobs/job/template.py
index fa73ad58..576e14f9 100644
--- a/pyiron_base/jobs/job/template.py
+++ b/pyiron_base/jobs/job/template.py
@@ -24,7 +24,7 @@ __date__ = "May 15, 2020"
 class TemplateJob(GenericJob, HasStorage):
     def __init__(self, project, job_name):
         GenericJob.__init__(self, project, job_name)
-        HasStorage.__init__(self)
+        HasStorage.__init__(self, group_name="")
         self.storage.create_group("input")
         self.storage.create_group("output")
 
@@ -38,11 +38,11 @@ class TemplateJob(GenericJob, HasStorage):
 
     def to_hdf(self, hdf=None, group_name=None):
         GenericJob.to_hdf(self, hdf=hdf, group_name=group_name)
-        HasStorage.to_hdf(self, hdf=self.project_hdf5, group_name="")
+        HasStorage.to_hdf(self, hdf=self.project_hdf5)
 
     def from_hdf(self, hdf=None, group_name=None):
         GenericJob.from_hdf(self, hdf=hdf, group_name=group_name)
-        HasStorage.from_hdf(self, hdf=self.project_hdf5, group_name="")
+        HasStorage.from_hdf(self, hdf=self.project_hdf5)
 
 
 class PythonTemplateJob(TemplateJob):
diff --git a/pyiron_base/storage/datacontainer.py b/pyiron_base/storage/datacontainer.py
index b33fbeca..21004574 100644
--- a/pyiron_base/storage/datacontainer.py
+++ b/pyiron_base/storage/datacontainer.py
@@ -790,16 +790,22 @@ class DataContainer(MutableMapping, HasGroups, HasHDF):
 
     def _to_hdf(self, hdf):
         hdf["READ_ONLY"] = self.read_only
+        written_keys = _internal_hdf_nodes.copy()
         for i, (k, v) in enumerate(self.items()):
             if isinstance(k, str) and "__index_" in k:
                 raise ValueError("Key {} clashes with internal use!".format(k))
 
             k = "{}__index_{}".format(k if isinstance(k, str) else "", i)
+            written_keys.append(k)
 
             # pandas objects also have a to_hdf method that is entirely unrelated to ours
             if hasattr(v, "to_hdf") and not isinstance(
                 v, (pandas.DataFrame, pandas.Series)
             ):
+                # if v will be written as a group, but a node of the same name k exists already in the file, h5py will
+                # complain, so delete it first
+                if k in hdf.list_nodes():
+                    del hdf[k]
                 v.to_hdf(hdf=hdf, group_name=k)
             else:
                 # if the value doesn't know how to serialize itself, assume
@@ -811,6 +817,12 @@ class DataContainer(MutableMapping, HasGroups, HasHDF):
                         "Error saving {} (key {}): DataContainer doesn't support saving elements "
                         'of type "{}" to HDF!'.format(v, k, type(v))
                     ) from None
+        for n in hdf.list_nodes():
+            if n not in written_keys:
+                del hdf[n]
+        for g in hdf.list_groups():
+            if g not in written_keys:
+                del hdf[g]
 
     def _from_hdf(self, hdf, version=None):
         self.clear()
@@ -916,7 +928,7 @@ class DataContainer(MutableMapping, HasGroups, HasHDF):
         if not self._lazy and not recursive:
             return
 
-        # values are loaded from HDF once they are accessed via __getitem__, which is implicetly called by values()
+        # values are loaded from HDF once they are accessed via __getitem__, which is implicitly called by values()
         for v in self.values():
             if recursive and isinstance(v, DataContainer):
                 v._force_load()
diff --git a/pyiron_base/storage/hdfio.py b/pyiron_base/storage/hdfio.py
index 5e787c3c..198dc767 100644
--- a/pyiron_base/storage/hdfio.py
+++ b/pyiron_base/storage/hdfio.py
@@ -260,7 +260,7 @@ class FileHDFio(HasGroups, MutableMapping):
         h5io.write_hdf5(
             self.file_name,
             value,
-            title=posixpath.join(self.h5_path, key),
+            title=self._get_h5_path(key),
             overwrite="update",
             use_json=use_json,
         )
@@ -275,7 +275,7 @@ class FileHDFio(HasGroups, MutableMapping):
         if self.file_exists:
             try:
                 with open_hdf5(self.file_name, mode="a") as store:
-                    del store[key]
+                    del store[self._get_h5_path(key)]
             except (AttributeError, KeyError):
                 pass
 
@@ -531,7 +531,7 @@ class FileHDFio(HasGroups, MutableMapping):
         Returns:
             FileHDFio: FileHDFio object pointing to the new group
         """
-        full_name = posixpath.join(self.h5_path, name)
+        full_name = self._get_h5_path(name)
         with open_hdf5(self.file_name, mode="a") as h:
             try:
                 h.create_group(full_name, track_order=track_order)
@@ -570,7 +570,7 @@ class FileHDFio(HasGroups, MutableMapping):
         if h5_rel_path.strip() == ".":
             h5_rel_path = ""
         if h5_rel_path.strip() != "":
-            new_h5_path.h5_path = posixpath.join(new_h5_path.h5_path, h5_rel_path)
+            new_h5_path.h5_path = self._get_h5_path(h5_rel_path)
         new_h5_path.history.append(h5_rel_path)
 
         return new_h5_path
