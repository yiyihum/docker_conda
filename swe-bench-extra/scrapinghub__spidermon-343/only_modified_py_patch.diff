diff --git a/examples/tutorial/tutorial/settings.py b/examples/tutorial/tutorial/settings.py
index b6f9b92..61cc8e2 100644
--- a/examples/tutorial/tutorial/settings.py
+++ b/examples/tutorial/tutorial/settings.py
@@ -19,9 +19,7 @@ SPIDERMON_VALIDATION_MODELS = ("tutorial.validators.QuoteItem",)
 
 SPIDERMON_VALIDATION_ADD_ERRORS_TO_ITEMS = True
 
-STATS_CLASS = (
-    "spidermon.contrib.stats.statscollectors.LocalStorageStatsHistoryCollector"
-)
+STATS_CLASS = "spidermon.contrib.stats.statscollectors.local_storage.LocalStorageStatsHistoryCollector"
 
 SPIDERMON_MAX_STORED_STATS = 10  # Stores the stats of the last 10 spider execution
 
diff --git a/spidermon/contrib/stats/statscollectors.py b/spidermon/contrib/stats/statscollectors/sc_collections.py
similarity index 60%
rename from spidermon/contrib/stats/statscollectors.py
rename to spidermon/contrib/stats/statscollectors/sc_collections.py
index 13cf7cb..2ca98b0 100644
--- a/spidermon/contrib/stats/statscollectors.py
+++ b/spidermon/contrib/stats/statscollectors/sc_collections.py
@@ -1,48 +1,14 @@
 import logging
 import os
-import pickle
 from collections import deque
 
 import scrapinghub
-from scrapy.statscollectors import StatsCollector
-from scrapy.utils.project import data_path
 from sh_scrapy.stats import HubStorageStatsCollector
 
 logger = logging.getLogger(__name__)
 
 
-class LocalStorageStatsHistoryCollector(StatsCollector):
-    def _stats_location(self, spider):
-        statsdir = data_path("stats", createdir=True)
-        return os.path.join(statsdir, f"{spider.name}_stats_history")
-
-    def open_spider(self, spider):
-        stats_location = self._stats_location(spider)
-
-        max_stored_stats = spider.crawler.settings.getint(
-            "SPIDERMON_MAX_STORED_STATS", default=100
-        )
-
-        if os.path.isfile(stats_location):
-            with open(stats_location, "rb") as stats_file:
-                _stats_history = pickle.load(stats_file)
-        else:
-            _stats_history = deque([], maxlen=max_stored_stats)
-
-        if _stats_history.maxlen != max_stored_stats:
-            _stats_history = deque(_stats_history, maxlen=max_stored_stats)
-
-        spider.stats_history = _stats_history
-
-    def _persist_stats(self, stats, spider):
-        stats_location = self._stats_location(spider)
-
-        spider.stats_history.appendleft(self._stats)
-        with open(stats_location, "wb") as stats_file:
-            pickle.dump(spider.stats_history, stats_file)
-
-
-class DashCollectionsStatsHistoryCollector(HubStorageStatsCollector):
+class ScrapyCloudCollectionsStatsHistoryCollector(HubStorageStatsCollector):
     def _open_collection(self, spider):
         sh_client = scrapinghub.ScrapinghubClient()
         proj_id = os.environ.get("SCRAPY_PROJECT_ID")

