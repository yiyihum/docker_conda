diff --git a/tests/contrib/stats/statscollectors/__init__.py b/tests/contrib/stats/statscollectors/__init__.py
new file mode 100644
index 0000000..e69de29
diff --git a/tests/contrib/stats/test_localstoragestats.py b/tests/contrib/stats/statscollectors/test_local_storage.py
similarity index 79%
rename from tests/contrib/stats/test_localstoragestats.py
rename to tests/contrib/stats/statscollectors/test_local_storage.py
index bc5d488..1e117cb 100644
--- a/tests/contrib/stats/test_localstoragestats.py
+++ b/tests/contrib/stats/statscollectors/test_local_storage.py
@@ -5,7 +5,9 @@ import pytest
 
 from scrapy import Spider
 from scrapy.utils.test import get_crawler
-from spidermon.contrib.stats.statscollectors import LocalStorageStatsHistoryCollector
+from spidermon.contrib.stats.statscollectors.local_storage import (
+    LocalStorageStatsHistoryCollector,
+)
 
 
 @pytest.fixture
@@ -21,7 +23,7 @@ def stats_temporary_location(monkeypatch, tmp_path):
 def test_settings():
     return {
         "STATS_CLASS": (
-            "spidermon.contrib.stats.statscollectors.LocalStorageStatsHistoryCollector"
+            "spidermon.contrib.stats.statscollectors.local_storage.LocalStorageStatsHistoryCollector"
         )
     }
 
@@ -126,3 +128,25 @@ def test_spider_limit_number_of_stored_stats(test_settings, stats_temporary_loca
     assert "third_execution" in crawler.spider.stats_history[0].keys()
     assert "second_execution" in crawler.spider.stats_history[1].keys()
     crawler.stop()
+
+
+def test_able_to_import_deprecated_local_storage_stats_collector_module():
+    """
+    To avoid an error when importing this stats collector with the old location
+    in legacy code, we need to ensure that LocalStorageStatsHistoryCollector can
+    be imported as the old module.
+
+    Original module:
+    spidermon.contrib.stats.statscollectors.LocalStorageStatsHistoryCollector
+
+    New module:
+    spidermon.contrib.stats.statscollectors.local_storage.LocalStorageStatsHistoryCollector
+    """
+    try:
+        from spidermon.contrib.stats.statscollectors import (
+            LocalStorageStatsHistoryCollector,
+        )
+    except ModuleNotFoundError:
+        assert (
+            False
+        ), f"Unable to import spidermon.contrib.stats.statscollectors.LocalStorageStatsHistoryCollector"
diff --git a/tests/contrib/stats/test_dashcollectionsstats.py b/tests/contrib/stats/statscollectors/test_sc_collections.py
similarity index 84%
rename from tests/contrib/stats/test_dashcollectionsstats.py
rename to tests/contrib/stats/statscollectors/test_sc_collections.py
index 232c5f7..5741959 100644
--- a/tests/contrib/stats/test_dashcollectionsstats.py
+++ b/tests/contrib/stats/statscollectors/test_sc_collections.py
@@ -6,7 +6,9 @@ import scrapinghub
 
 from scrapy import Spider
 from scrapy.utils.test import get_crawler
-from spidermon.contrib.stats.statscollectors import DashCollectionsStatsHistoryCollector
+from spidermon.contrib.stats.statscollectors.sc_collections import (
+    ScrapyCloudCollectionsStatsHistoryCollector,
+)
 
 
 class StoreMock:
@@ -27,7 +29,7 @@ class StoreMock:
 def stats_collection(monkeypatch):
     store = StoreMock()
     monkeypatch.setattr(
-        DashCollectionsStatsHistoryCollector,
+        ScrapyCloudCollectionsStatsHistoryCollector,
         "_open_collection",
         lambda *args: store,
     )
@@ -38,7 +40,7 @@ def stats_collection_not_exist(monkeypatch):
     store = StoreMock()
     store.raise_iter_error = True
     monkeypatch.setattr(
-        DashCollectionsStatsHistoryCollector,
+        ScrapyCloudCollectionsStatsHistoryCollector,
         "_open_collection",
         lambda *args: store,
     )
@@ -48,31 +50,31 @@ def stats_collection_not_exist(monkeypatch):
 def test_settings():
     return {
         "STATS_CLASS": (
-            "spidermon.contrib.stats.statscollectors.DashCollectionsStatsHistoryCollector"
+            "spidermon.contrib.stats.statscollectors.sc_collections.ScrapyCloudCollectionsStatsHistoryCollector"
         )
     }
 
 
-@patch("spidermon.contrib.stats.statscollectors.scrapinghub")
+@patch("spidermon.contrib.stats.statscollectors.sc_collections.scrapinghub")
 def test_open_spider_without_api(scrapinghub_mock, test_settings):
     mock_spider = MagicMock()
     crawler = get_crawler(Spider, test_settings)
-    pipe = DashCollectionsStatsHistoryCollector(crawler)
+    pipe = ScrapyCloudCollectionsStatsHistoryCollector(crawler)
 
     pipe.open_spider(mock_spider)
 
     assert pipe.store is None
 
 
-@patch("spidermon.contrib.stats.statscollectors.scrapinghub")
-@patch("spidermon.contrib.stats.statscollectors.os.environ.get")
-def test_open_collection_with_api(scrapinghub_mock, os_enviorn_mock, test_settings):
+@patch("spidermon.contrib.stats.statscollectors.sc_collections.scrapinghub")
+@patch("spidermon.contrib.stats.statscollectors.sc_collections.os.environ.get")
+def test_open_collection_with_api(scrapinghub_mock, os_environ_mock, test_settings):
     mock_spider = MagicMock()
     mock_spider.name = "test"
 
-    os_enviorn_mock.return_value = 1234
+    os_environ_mock.return_value = 1234
     crawler = get_crawler(Spider, test_settings)
-    pipe = DashCollectionsStatsHistoryCollector(crawler)
+    pipe = ScrapyCloudCollectionsStatsHistoryCollector(crawler)
 
     store = pipe._open_collection(mock_spider)
 
@@ -181,7 +183,7 @@ def test_spider_limit_number_of_stored_stats(test_settings, stats_collection):
     crawler.stop()
 
 
-@patch("spidermon.contrib.stats.statscollectors.os.environ.get")
+@patch("spidermon.contrib.stats.statscollectors.sc_collections.os.environ.get")
 def test_job_id_added(mock_os_enviorn_get, test_settings, stats_collection):
     mock_os_enviorn_get.return_value = "test/test/test"
     crawler = get_crawler(Spider, test_settings)
@@ -195,7 +197,7 @@ def test_job_id_added(mock_os_enviorn_get, test_settings, stats_collection):
     )
 
 
-@patch("spidermon.contrib.stats.statscollectors.os.environ.get")
+@patch("spidermon.contrib.stats.statscollectors.sc_collections.os.environ.get")
 def test_job_id_not_available(mock_os_enviorn_get, test_settings, stats_collection):
     mock_os_enviorn_get.return_value = None
     crawler = get_crawler(Spider, test_settings)
@@ -206,7 +208,7 @@ def test_job_id_not_available(mock_os_enviorn_get, test_settings, stats_collecti
     assert "job_url" not in crawler.spider.stats_history[0]
 
 
-@patch("spidermon.contrib.stats.statscollectors.os.environ.get")
+@patch("spidermon.contrib.stats.statscollectors.sc_collections.os.environ.get")
 def test_stats_history_when_no_collection(
     os_enviorn_mock, stats_collection_not_exist, test_settings
 ):
@@ -216,6 +218,6 @@ def test_stats_history_when_no_collection(
 
     os_enviorn_mock.return_value = 1234
     crawler = get_crawler(Spider, test_settings)
-    pipe = DashCollectionsStatsHistoryCollector(crawler)
+    pipe = ScrapyCloudCollectionsStatsHistoryCollector(crawler)
     pipe.open_spider(mock_spider)
     assert mock_spider.stats_history == deque([], maxlen=100)
