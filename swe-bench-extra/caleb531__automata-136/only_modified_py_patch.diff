diff --git a/automata/regex/parser.py b/automata/regex/parser.py
index 61f0e10..098e3cb 100644
--- a/automata/regex/parser.py
+++ b/automata/regex/parser.py
@@ -492,8 +492,9 @@ def val(self) -> NFARegexBuilder:
         return NFARegexBuilder.wildcard(self.input_symbols, self.counter)
 
 
-def add_concat_tokens(
+def add_concat_and_empty_string_tokens(
     token_list: List[Token[NFARegexBuilder]],
+    state_name_counter: count,
 ) -> List[Token[NFARegexBuilder]]:
     """Add concat tokens to list of parsed infix tokens."""
 
@@ -509,6 +510,9 @@ def add_concat_tokens(
         (PostfixOperator, LeftParen),
     ]
 
+    # Pairs of tokens to insert empty string literals between
+    empty_string_pairs = [(LeftParen, RightParen)]
+
     for curr_token, next_token in zip_longest(token_list, token_list[1:]):
         final_token_list.append(curr_token)
 
@@ -519,13 +523,20 @@ def add_concat_tokens(
                 ):
                     final_token_list.append(ConcatToken(""))
 
+            for firstClass, secondClass in empty_string_pairs:
+                if isinstance(curr_token, firstClass) and isinstance(
+                    next_token, secondClass
+                ):
+                    final_token_list.append(StringToken("", state_name_counter))
+
     return final_token_list
 
 
-def get_regex_lexer(input_symbols: AbstractSet[str]) -> Lexer[NFARegexBuilder]:
+def get_regex_lexer(
+    input_symbols: AbstractSet[str], state_name_counter: count
+) -> Lexer[NFARegexBuilder]:
     """Get lexer for parsing regular expressions."""
     lexer: Lexer[NFARegexBuilder] = Lexer()
-    state_name_counter = count(0)
 
     lexer.register_token(LeftParen.from_match, r"\(")
     lexer.register_token(RightParen.from_match, r"\)")
@@ -553,10 +564,14 @@ def parse_regex(regexstr: str, input_symbols: AbstractSet[str]) -> NFARegexBuild
     if len(regexstr) == 0:
         return NFARegexBuilder.from_string_literal(regexstr, count(0))
 
-    lexer = get_regex_lexer(input_symbols)
+    state_name_counter = count(0)
+
+    lexer = get_regex_lexer(input_symbols, state_name_counter)
     lexed_tokens = lexer.lex(regexstr)
     validate_tokens(lexed_tokens)
-    tokens_with_concats = add_concat_tokens(lexed_tokens)
+    tokens_with_concats = add_concat_and_empty_string_tokens(
+        lexed_tokens, state_name_counter
+    )
     postfix = tokens_to_postfix(tokens_with_concats)
 
     return parse_postfix_tokens(postfix)
diff --git a/automata/regex/postfix.py b/automata/regex/postfix.py
index a9ab999..2c173ea 100644
--- a/automata/regex/postfix.py
+++ b/automata/regex/postfix.py
@@ -103,7 +103,7 @@ def validate_tokens(token_list: List[Token]) -> None:
         # No left parens right before infix or postfix operators, or right
         # before a right paren
         elif isinstance(prev_token, LeftParen):
-            if isinstance(curr_token, (InfixOperator, PostfixOperator, RightParen)):
+            if isinstance(curr_token, (InfixOperator, PostfixOperator)):
                 raise exceptions.InvalidRegexError(
                     f"'{prev_token}' cannot appear immediately before '{curr_token}'."
                 )
diff --git a/automata/regex/regex.py b/automata/regex/regex.py
index 1fa02b6..9c9414c 100644
--- a/automata/regex/regex.py
+++ b/automata/regex/regex.py
@@ -1,6 +1,7 @@
 #!/usr/bin/env python3
 """Methods for working with regular expressions"""
 
+from itertools import count
 from typing import AbstractSet, Literal, Optional
 
 import automata.base.exceptions as exceptions
@@ -23,7 +24,7 @@ def validate(regex: str) -> Literal[True]:
     """Raise an error if the regular expression is invalid"""
     input_symbols = set(regex) - RESERVED_CHARACTERS
 
-    validate_tokens(get_regex_lexer(input_symbols).lex(regex))
+    validate_tokens(get_regex_lexer(input_symbols, count(0)).lex(regex))
 
     return True
 
