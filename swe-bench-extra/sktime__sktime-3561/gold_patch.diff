diff --git a/sktime/annotation/hmm_learn/base.py b/sktime/annotation/hmm_learn/base.py
index e688bdd16..c4b6fb0da 100644
--- a/sktime/annotation/hmm_learn/base.py
+++ b/sktime/annotation/hmm_learn/base.py
@@ -22,7 +22,7 @@ __all__ = ["BaseHMMLearn"]
 class BaseHMMLearn(BaseSeriesAnnotator):
     """Base class for all HMM wrappers, handles required overlap between packages."""
 
-    __tags = {
+    _tags = {
         "univariate-only": True,
         "fit_is_empty": True,
         "python_dependencies": "hmmlearn",
diff --git a/sktime/regression/deep_learning/cnn.py b/sktime/regression/deep_learning/cnn.py
index 37d7020af..d7773ef7c 100644
--- a/sktime/regression/deep_learning/cnn.py
+++ b/sktime/regression/deep_learning/cnn.py
@@ -1,9 +1,11 @@
 # -*- coding: utf-8 -*-
 """Time Convolutional Neural Network (CNN) for regression."""
 
-__author__ = ["AurumnPegasus"]
+__author__ = ["AurumnPegasus", "achieveordie"]
 __all__ = ["CNNRegressor"]
 
+from sklearn.utils import check_random_state
+
 from sktime.networks.cnn import CNNNetwork
 from sktime.regression.deep_learning.base import BaseDeepRegressor
 from sktime.utils.validation._dependencies import _check_dl_dependencies
@@ -34,7 +36,12 @@ class CNNRegressor(BaseDeepRegressor):
         whether to output extra information
     loss            : string, default="mean_squared_error"
         fit parameter for the keras model
-    optimizer       : keras.optimizer, default=keras.optimizers.Adam(),
+    activation      : keras.activations or string, default `linear`
+        function to use in the output layer.
+    optimizer       : keras.optimizers or string, default `None`.
+        when `None`, internally uses `keras.optimizers.Adam(0.01)`
+    use_bias        : bool, default=True
+        whether to use bias in the output layer.
     metrics         : list of strings, default=["accuracy"],
 
     Notes
@@ -58,7 +65,10 @@ class CNNRegressor(BaseDeepRegressor):
         verbose=False,
         loss="mean_squared_error",
         metrics=None,
-        random_seed=0,
+        random_state=0,
+        activation="linear",
+        use_bias=True,
+        optimizer=None,
     ):
         _check_dl_dependencies(severity="error")
         super(CNNRegressor, self).__init__(
@@ -73,7 +83,11 @@ class CNNRegressor(BaseDeepRegressor):
         self.verbose = verbose
         self.loss = loss
         self.metrics = metrics
-        self.random_seed = random_seed
+        self.random_state = random_state
+        self.activation = activation
+        self.use_bias = use_bias
+        self.optimizer = optimizer
+        self.history = None
         self._network = CNNNetwork()
 
     def build_model(self, input_shape, **kwargs):
@@ -96,7 +110,7 @@ class CNNRegressor(BaseDeepRegressor):
         import tensorflow as tf
         from tensorflow import keras
 
-        tf.random.set_seed(self.random_seed)
+        tf.random.set_seed(self.random_state)
 
         if self.metrics is None:
             metrics = ["accuracy"]
@@ -105,13 +119,23 @@ class CNNRegressor(BaseDeepRegressor):
 
         input_layer, output_layer = self._network.build_network(input_shape, **kwargs)
 
-        output_layer = keras.layers.Dense(units=1, activation="sigmoid")(output_layer)
+        output_layer = keras.layers.Dense(
+            units=1,
+            activation=self.activation,
+            use_bias=self.use_bias,
+        )(output_layer)
+
+        self.optimizer_ = (
+            keras.optimizers.Adam(learning_rate=0.01)
+            if self.optimizer is None
+            else self.optimizer
+        )
 
         model = keras.models.Model(inputs=input_layer, outputs=output_layer)
 
         model.compile(
             loss=self.loss,
-            optimizer=keras.optimizers.Adam(),
+            optimizer=self.optimizer_,
             metrics=metrics,
         )
         return model
@@ -136,6 +160,7 @@ class CNNRegressor(BaseDeepRegressor):
         # Transpose to conform to Keras input style.
         X = X.transpose(0, 2, 1)
 
+        check_random_state(self.random_state)
         self.input_shape = X.shape[1:]
         self.model_ = self.build_model(self.input_shape)
         if self.verbose:
diff --git a/sktime/transformations/hierarchical/aggregate.py b/sktime/transformations/hierarchical/aggregate.py
index 9e32e6482..79fb305e2 100644
--- a/sktime/transformations/hierarchical/aggregate.py
+++ b/sktime/transformations/hierarchical/aggregate.py
@@ -147,7 +147,7 @@ class Aggregator(BaseTransformer):
                 df_out.reset_index(level=-1).loc[new_index].set_index(nm, append=True)
             ).rename_axis(X.index.names, axis=0)
 
-        df_out.sort_index(inplace=True)
+        df_out = df_out.sort_index()
 
         return df_out
 
diff --git a/sktime/utils/sampling.py b/sktime/utils/sampling.py
index de6f7dcab..bfe8581cf 100644
--- a/sktime/utils/sampling.py
+++ b/sktime/utils/sampling.py
@@ -26,7 +26,7 @@ def stratified_resample(X_train, y_train, X_test, y_test, random_state):
     X_test : pd.DataFrame
         test data attributes in sktime pandas format.
     y_test : np.array
-        test data class labes as np array.
+        test data class labels as np array.
     random_state : int
         seed to enable reproducable resamples
     Returns
@@ -71,12 +71,9 @@ def stratified_resample(X_train, y_train, X_test, y_test, random_state):
         X_test = pd.concat([X_test, test_instances])
         y_train = np.concatenate([y_train, train_labels], axis=None)
         y_test = np.concatenate([y_test, test_labels], axis=None)
-    # get the counts of the new train and test resample
-    unique_train_new, counts_train_new = np.unique(y_train, return_counts=True)
-    unique_test_new, counts_test_new = np.unique(y_test, return_counts=True)
-    # make sure they match the original distribution of data
-    assert list(counts_train_new) == list(counts_train)
-    assert list(counts_test_new) == list(counts_test)
+    # reset indexes to conform to sktime format.
+    X_train = X_train.reset_index(drop=True)
+    X_test = X_test.reset_index(drop=True)
     return X_train, y_train, X_test, y_test
 
 
